{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Event Data\n",
    "This notebook performs preprocessing for NYC event dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/31 17:58:46 WARN Utils: Your hostname, Hanshis-Laptop.local resolves to a loopback address: 127.0.0.1; using 10.12.218.66 instead (on interface en0)\n",
      "24/08/31 17:58:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/31 17:58:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/31 17:58:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to your CSV file\n",
    "csv_file_path = \"../data/landing/external/NYC_Permitted_Event_Information_-_Historical.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "edf = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Event ID: integer (nullable = true)\n",
      " |-- Event Name: string (nullable = true)\n",
      " |-- Start Date/Time: string (nullable = true)\n",
      " |-- End Date/Time: string (nullable = true)\n",
      " |-- Event Agency: string (nullable = true)\n",
      " |-- Event Type: string (nullable = true)\n",
      " |-- Event Borough: string (nullable = true)\n",
      " |-- Event Location: string (nullable = true)\n",
      " |-- Event Street Side: string (nullable = true)\n",
      " |-- Street Closure Type: string (nullable = true)\n",
      " |-- Community Board: string (nullable = true)\n",
      " |-- Police Precinct: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame schema\n",
    "edf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns: Event ID, Event Type, Start Date/Time, End Date/Time, Event Borough, Event Location\n",
    "edf = edf.select(\"Event ID\", \"Event Type\", \"Start Date/Time\", \"End Date/Time\", \"Event Borough\", \"Event Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns to datetime\n",
    "edf = edf.withColumn(\"Start Date/Time\", to_timestamp(\"Start Date/Time\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "edf = edf.withColumn(\"End Date/Time\", to_timestamp(\"End Date/Time\", \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================================> (38 + 1) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original count: 23827581\n",
      "Filtered count: 3340814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# Define the start and end dates as strings\n",
    "START_DATE = \"2023-07-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "\n",
    "# Convert start_date and end_date strings to timestamp literals\n",
    "start_timestamp = lit(START_DATE).cast(\"timestamp\")\n",
    "end_timestamp = lit(END_DATE).cast(\"timestamp\")\n",
    "\n",
    "# Filter the DataFrame to include only rows within the specified date range\n",
    "edf_filtered = edf.filter(\n",
    "    (col(\"Start Date/Time\") >= start_timestamp) &\n",
    "    (col(\"End Date/Time\") <= end_timestamp)\n",
    ")\n",
    "\n",
    "# Display the number of rows before and after filtering\n",
    "original_count = edf.count()\n",
    "filtered_count = edf_filtered.count()\n",
    "\n",
    "print(f\"Original count: {original_count}\")\n",
    "print(f\"Filtered count: {filtered_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===============================================>        (33 + 6) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data left: 14.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check the percetge of data left\n",
    "print(f\"Percentage of data left: {edf_filtered.count()/edf.count()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour and date from Start Date/Time\n",
    "edf_filtered = edf_filtered.withColumn(\"Start Hour\", hour(\"Start Date/Time\")) \\\n",
    "                           .withColumn(\"Start Date\", to_date(\"Start Date/Time\"))\n",
    "# Extract hour and date from End Date/Time\n",
    "edf_filtered = edf_filtered.withColumn(\"End Hour\", hour(\"End Date/Time\")) \\\n",
    "                           .withColumn(\"End Date\", to_date(\"End Date/Time\"))\n",
    "# Drop the Start Date/Time and End Date/Time columns\n",
    "edf_filtered = edf_filtered.drop(\"Start Date/Time\", \"End Date/Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns\n",
    "edf_filtered = edf_filtered.select(\"Event ID\",  \"Start Date\", \"Start Hour\", \"End Date\", \"End Hour\", \"Event Type\", \"Event Borough\", \"Event Location\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:============================================>           (11 + 3) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|Event ID|Start Date|Start Hour|  End Date|End Hour|   Event Type|Event Borough|      Event Location|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|  724241|2023-07-05|        11|2023-07-05|      14|Sport - Adult|     Brooklyn|Parade Ground: Ba...|\n",
      "|  724241|2023-07-05|        11|2023-07-05|      14|Sport - Adult|     Brooklyn|Parade Ground: Ba...|\n",
      "|  724241|2023-07-05|        11|2023-07-05|      14|Sport - Adult|     Brooklyn|Parade Ground: Ba...|\n",
      "|  679641|2023-08-11|         0|2023-08-11|      23|Special Event|    Manhattan|Central Park: Min...|\n",
      "|  679665|2023-08-21|         0|2023-08-21|      23|Special Event|    Manhattan|Central Park: Ced...|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame \n",
    "edf_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取event，试图转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the location details from the Event Location column only before \"\":\" character\n",
    "# Split the Event Location column by the \":\" character\n",
    "# Extract the first part of the split\n",
    "# Rename the column to \"Location Details\"\n",
    "# Drop the Event Location column\n",
    "# Show the DataFrame \n",
    "edf_filtered = edf_filtered.withColumn(\"Location Details\", split(\"Event Location\", \":\").getItem(0)) \\\n",
    "                           .drop(\"Event Location\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:============================================>           (11 + 3) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-------------+-------------+----------------+\n",
      "|Event ID|Start Date|Start Hour|  End Date|End Hour|   Event Type|Event Borough|Location Details|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+----------------+\n",
      "|  724241|2023-07-05|        11|2023-07-05|      14|Sport - Adult|     Brooklyn|   Parade Ground|\n",
      "|  724241|2023-07-05|        11|2023-07-05|      14|Sport - Adult|     Brooklyn|   Parade Ground|\n",
      "|  724241|2023-07-05|        11|2023-07-05|      14|Sport - Adult|     Brooklyn|   Parade Ground|\n",
      "|  679641|2023-08-11|         0|2023-08-11|      23|Special Event|    Manhattan|    Central Park|\n",
      "|  679665|2023-08-21|         0|2023-08-21|      23|Special Event|    Manhattan|    Central Park|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "edf_filtered.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Export the filtered DataFrame to a Parquet file in the raw zone\n",
    "edf_filtered.write.mode(\"overwrite\").parquet(\"../data/raw/NYC_Permitted_Event_Information_Historical.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
