{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Event Data\n",
    "This notebook performs preprocessing for NYC event dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to your CSV file\n",
    "csv_file_path = \"../data/landing/external/NYC_Permitted_Event_Information_-_Historical.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "edf = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Event ID: integer (nullable = true)\n",
      " |-- Event Name: string (nullable = true)\n",
      " |-- Start Date/Time: string (nullable = true)\n",
      " |-- End Date/Time: string (nullable = true)\n",
      " |-- Event Agency: string (nullable = true)\n",
      " |-- Event Type: string (nullable = true)\n",
      " |-- Event Borough: string (nullable = true)\n",
      " |-- Event Location: string (nullable = true)\n",
      " |-- Event Street Side: string (nullable = true)\n",
      " |-- Street Closure Type: string (nullable = true)\n",
      " |-- Community Board: string (nullable = true)\n",
      " |-- Police Precinct: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame schema\n",
    "edf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns: Event ID, Event Type, Start Date/Time, End Date/Time, Event Borough, Event Location\n",
    "edf = edf.select(\"Event ID\", \"Event Type\", \"Start Date/Time\", \"End Date/Time\", \"Event Borough\", \"Event Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit\n",
    "\n",
    "# Convert the 'Start Date/Time' and 'End Date/Time' columns to timestamp type using the correct format\n",
    "edf = edf.withColumn(\"Start Date/Time\", to_timestamp(col(\"Start Date/Time\"), \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "       .withColumn(\"End Date/Time\", to_timestamp(col(\"End Date/Time\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Define the date range as timestamps in the correct format\n",
    "start_date = \"2023-12-01 00:00:00\"\n",
    "end_date = \"2024-05-31 23:59:59\"\n",
    "\n",
    "# Convert start_date and end_date to timestamp using lit and to_timestamp\n",
    "start_timestamp = to_timestamp(lit(start_date), \"yyyy-MM-dd HH:mm:ss\")\n",
    "end_timestamp = to_timestamp(lit(end_date), \"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "# Filter the DataFrame to include only rows within the specified date range\n",
    "edf_filtered = edf.filter((col(\"Start Date/Time\") >= start_timestamp) & \n",
    "                          (col(\"End Date/Time\") <= end_timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values and duplicates \n",
    "edf_filtered = edf_filtered.dropna()\n",
    "edf_filtered = edf_filtered.dropDuplicates()\n",
    "# Checked and ther's no missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour and date from Start Date/Time\n",
    "edf_filtered = edf_filtered.withColumn(\"Start Hour\", hour(\"Start Date/Time\")) \\\n",
    "                           .withColumn(\"Start Date\", to_date(\"Start Date/Time\"))\n",
    "# Extract hour and date from End Date/Time\n",
    "edf_filtered = edf_filtered.withColumn(\"End Hour\", hour(\"End Date/Time\")) \\\n",
    "                           .withColumn(\"End Date\", to_date(\"End Date/Time\"))\n",
    "# Drop the Start Date/Time and End Date/Time columns\n",
    "edf_filtered = edf_filtered.drop(\"Start Date/Time\", \"End Date/Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns\n",
    "edf_filtered = edf_filtered.select(\"Event ID\",  \"Start Date\", \"Start Hour\", \"End Date\", \"End Hour\", \"Event Type\", \"Event Borough\", \"Event Location\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=35542Kb max_used=35542Kb free=95529Kb\n",
      " bounds [0x00000001081e8000, 0x000000010a4e8000, 0x00000001101e8000]\n",
      " total_blobs=13201 nmethods=12261 adapters=851\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:================================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|Event ID|Start Date|Start Hour|  End Date|End Hour|   Event Type|Event Borough|      Event Location|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|  684438|2023-12-09|         7|2023-12-09|      10|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684437|2023-12-02|         7|2023-12-02|      10|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684438|2023-12-09|         7|2023-12-09|      10|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684437|2023-12-02|         7|2023-12-02|      10|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684438|2023-12-09|         7|2023-12-09|      10|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame \n",
    "edf_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Export the filtered DataFrame to a Parquet file in the raw zone\n",
    "edf_filtered.write.mode(\"overwrite\").parquet(\"../data/raw/NYC_Permitted_Event_Information_Historical.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
