{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 18:05:58 WARN Utils: Your hostname, Hanshis-Laptop.local resolves to a loopback address: 127.0.0.1; using 10.12.200.32 instead (on interface en0)\n",
      "24/08/26 18:05:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/26 18:05:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to your CSV file\n",
    "csv_file_path = \"../data/landing/external/NYC_Permitted_Event_Information.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "edf = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Event ID: integer (nullable = true)\n",
      " |-- Event Type: string (nullable = true)\n",
      " |-- Start Date/Time: string (nullable = true)\n",
      " |-- End Date/Time: string (nullable = true)\n",
      " |-- Event Borough: string (nullable = true)\n",
      " |-- Event Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame schema\n",
    "edf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+--------------------+-------------+--------------------+\n",
      "|Event ID|    Event Type|     Start Date/Time|       End Date/Time|Event Borough|      Event Location|\n",
      "+--------+--------------+--------------------+--------------------+-------------+--------------------+\n",
      "|  368421| Special Event|11/18/2017 07:00:...|11/18/2017 08:00:...|    Manhattan|Damrosch Park: Da...|\n",
      "|  330050| Special Event|11/16/2017 08:00:...|11/16/2017 04:00:...|        Bronx|Mount Eden Malls:...|\n",
      "|  314111|Farmers Market|11/21/2017 08:00:...|11/21/2017 05:00:...|    Manhattan| BROADWAY between...|\n",
      "|  369850|  Construction|11/23/2017 12:00:...|11/23/2017 11:58:...|    Manhattan|Madison Square Pa...|\n",
      "|  335783| Special Event|11/22/2017 09:00:...|11/22/2017 08:00:...|Staten Island|LaTourette Park &...|\n",
      "+--------+--------------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows\n",
    "edf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+--------------------+-------------+--------------------+\n",
      "|Event ID|    Event Type|     Start Date/Time|       End Date/Time|Event Borough|      Event Location|\n",
      "+--------+--------------+--------------------+--------------------+-------------+--------------------+\n",
      "|  368421| Special Event|11/18/2017 07:00:...|11/18/2017 08:00:...|    Manhattan|Damrosch Park: Da...|\n",
      "|  330050| Special Event|11/16/2017 08:00:...|11/16/2017 04:00:...|        Bronx|Mount Eden Malls:...|\n",
      "|  314111|Farmers Market|11/21/2017 08:00:...|11/21/2017 05:00:...|    Manhattan| BROADWAY between...|\n",
      "|  369850|  Construction|11/23/2017 12:00:...|11/23/2017 11:58:...|    Manhattan|Madison Square Pa...|\n",
      "|  335783| Special Event|11/22/2017 09:00:...|11/22/2017 08:00:...|Staten Island|LaTourette Park &...|\n",
      "+--------+--------------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the columns: Event ID, Event Type, Start Date/Time, End Date/Time, Event Borough, Event Location\n",
    "edf = edf.select(\"Event ID\", \"Event Type\", \"Start Date/Time\", \"End Date/Time\", \"Event Borough\", \"Event Location\")\n",
    "edf.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:================================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+-------------------+-------------+--------------------+\n",
      "|Event ID|   Event Type|    Start Date/Time|      End Date/Time|Event Borough|      Event Location|\n",
      "+--------+-------------+-------------------+-------------------+-------------+--------------------+\n",
      "|  684438|Special Event|2023-12-09 07:00:00|2023-12-09 10:00:00|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684437|Special Event|2023-12-02 07:00:00|2023-12-02 10:00:00|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684438|Special Event|2023-12-09 07:00:00|2023-12-09 10:00:00|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684437|Special Event|2023-12-02 07:00:00|2023-12-02 10:00:00|     Brooklyn|Prospect Park: Pi...|\n",
      "|  684438|Special Event|2023-12-09 07:00:00|2023-12-09 10:00:00|     Brooklyn|Prospect Park: Pi...|\n",
      "+--------+-------------+-------------------+-------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit\n",
    "\n",
    "# Convert the 'Start Date/Time' and 'End Date/Time' columns to timestamp type using the correct format\n",
    "edf = edf.withColumn(\"Start Date/Time\", to_timestamp(col(\"Start Date/Time\"), \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "       .withColumn(\"End Date/Time\", to_timestamp(col(\"End Date/Time\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Define the date range as timestamps in the correct format\n",
    "start_date = \"2023-12-01 00:00:00\"\n",
    "end_date = \"2024-05-31 23:59:59\"\n",
    "\n",
    "# Convert start_date and end_date to timestamp using lit and to_timestamp\n",
    "start_timestamp = to_timestamp(lit(start_date), \"yyyy-MM-dd HH:mm:ss\")\n",
    "end_timestamp = to_timestamp(lit(end_date), \"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "# Filter the DataFrame to include only rows within the specified date range\n",
    "edf_filtered = edf.filter((col(\"Start Date/Time\") >= start_timestamp) & \n",
    "                          (col(\"End Date/Time\") <= end_timestamp))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "edf_filtered.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:======================================================> (38 + 1) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+-------------+-------------+--------------+\n",
      "|Event ID|Event Type|Start Date/Time|End Date/Time|Event Borough|Event Location|\n",
      "+--------+----------+---------------+-------------+-------------+--------------+\n",
      "|       0|         0|              0|            0|            0|             0|\n",
      "+--------+----------+---------------+-------------+-------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check missing values \n",
    "edf_filtered.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in edf_filtered.columns]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
