{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Event Data\n",
    "This notebook performs preprocessing for NYC event dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=>                                                       (1 + 8) / 39]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to your CSV file\n",
    "csv_file_path = \"../data/landing/external/NYC_Permitted_Event_Information_-_Historical.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "edf = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Event ID: integer (nullable = true)\n",
      " |-- Event Name: string (nullable = true)\n",
      " |-- Start Date/Time: string (nullable = true)\n",
      " |-- End Date/Time: string (nullable = true)\n",
      " |-- Event Agency: string (nullable = true)\n",
      " |-- Event Type: string (nullable = true)\n",
      " |-- Event Borough: string (nullable = true)\n",
      " |-- Event Location: string (nullable = true)\n",
      " |-- Event Street Side: string (nullable = true)\n",
      " |-- Street Closure Type: string (nullable = true)\n",
      " |-- Community Board: string (nullable = true)\n",
      " |-- Police Precinct: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame schema\n",
    "edf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns: Event ID, Event Type, Start Date/Time, End Date/Time, Event Borough, Event Location\n",
    "edf = edf.select(\"Event ID\", \"Event Type\", \"Start Date/Time\", \"End Date/Time\", \"Event Borough\", \"Event Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns to datetime\n",
    "edf = edf.withColumn(\"Start Date/Time\", to_timestamp(\"Start Date/Time\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "edf = edf.withColumn(\"End Date/Time\", to_timestamp(\"End Date/Time\", \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=====================================================>  (37 + 2) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original count: 23827581\n",
      "Filtered count: 3340814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# Define the start and end dates as strings\n",
    "START_DATE = \"2023-07-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "\n",
    "# Convert start_date and end_date strings to timestamp literals\n",
    "start_timestamp = lit(START_DATE).cast(\"timestamp\")\n",
    "end_timestamp = lit(END_DATE).cast(\"timestamp\")\n",
    "\n",
    "# Filter the DataFrame to include only rows within the specified date range\n",
    "edf_filtered = edf.filter(\n",
    "    (col(\"Start Date/Time\") >= start_timestamp) &\n",
    "    (col(\"End Date/Time\") <= end_timestamp)\n",
    ")\n",
    "\n",
    "# Display the number of rows before and after filtering\n",
    "original_count = edf.count()\n",
    "filtered_count = edf_filtered.count()\n",
    "\n",
    "print(f\"Original count: {original_count}\")\n",
    "print(f\"Filtered count: {filtered_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:===============================================>        (33 + 6) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data left: 0.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check the percetge of data left\n",
    "print(f\"Percentage of data left: {edf_filtered.count()/edf.count()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour and date from Start Date/Time\n",
    "edf_filtered = edf_filtered.withColumn(\"Start Hour\", hour(\"Start Date/Time\")) \\\n",
    "                           .withColumn(\"Start Date\", to_date(\"Start Date/Time\"))\n",
    "# Extract hour and date from End Date/Time\n",
    "edf_filtered = edf_filtered.withColumn(\"End Hour\", hour(\"End Date/Time\")) \\\n",
    "                           .withColumn(\"End Date\", to_date(\"End Date/Time\"))\n",
    "# Drop the Start Date/Time and End Date/Time columns\n",
    "edf_filtered = edf_filtered.drop(\"Start Date/Time\", \"End Date/Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns\n",
    "edf_filtered = edf_filtered.select(\"Event ID\",  \"Start Date\", \"Start Hour\", \"End Date\", \"End Hour\", \"Event Type\", \"Event Borough\", \"Event Location\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:======================================================> (38 + 1) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|Event ID|Start Date|Start Hour|  End Date|End Hour|   Event Type|Event Borough|      Event Location|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|  682142|2023-07-08|         8|2023-07-08|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  679970|2023-07-29|        12|2023-07-29|      19|Special Event|     Brooklyn|Herbert Von King ...|\n",
      "|  682137|2023-07-01|         8|2023-07-01|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  683055|2023-10-28|         8|2023-10-28|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  683191|2023-11-24|         8|2023-11-24|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame \n",
    "edf_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Export the filtered DataFrame to a Parquet file in the raw zone\n",
    "edf_filtered.write.mode(\"overwrite\").parquet(\"../data/raw/NYC_Permitted_Event_Information_Historical.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
