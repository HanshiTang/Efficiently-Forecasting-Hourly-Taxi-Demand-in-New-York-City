{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing_final\n",
    "This notebook merges 3 preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from preprocessed tlc data\n",
    "tdf = spark.read.parquet(\"../data/raw/tlc_df.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed weather data\n",
    "wdf = spark.read.csv(\"../data/raw/NYC_weather_raw.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed event data\n",
    "edf = spark.read.parquet(\"../data/raw/NYC_Permitted_Event_Information_Historical.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|pickup_hour|dropoff_hour|pickup_date|dropoff_date|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|       2|              1|         1.75|         1|             false|          68|          50|           2|       12.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        17.1|                 2.5|      0.0|         10.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       1|              1|          3.6|         1|             false|          48|          43|           1|       17.7|  3.5|    0.5|      4.55|         0.0|                  1.0|       27.25|                 2.5|      0.0|         16.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|        10.28|         1|             false|         170|          95|           1|       44.3|  1.0|    0.5|     11.25|        6.94|                  1.0|       67.49|                 2.5|      0.0|         29.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|         2.66|         1|             false|         163|         158|           1|       14.9|  1.0|    0.5|      3.98|         0.0|                  1.0|       23.88|                 2.5|      0.0|         14.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|        15.43|         1|             false|         132|         150|           2|       59.0|  1.0|    0.5|       0.0|         0.0|                  1.0|       63.25|                 0.0|      0.0|         24.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the tlc data\n",
    "tdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- dropoff_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema of the tlc data\n",
    "tdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+---+------+---+----+------+\n",
      "|      DATE|HOUR|    CIG|WND|   VIS|TMP| DEW|   SLP|\n",
      "+----------+----+-------+---+------+---+----+------+\n",
      "|2023-12-01|   0|22000.0|4.1|1609.3|9.4|-2.8|1020.1|\n",
      "|2023-12-01|   1|22000.0|2.1|1609.3|8.9|-2.2|1020.0|\n",
      "|2023-12-01|   2|22000.0|3.1|1609.3|8.9|-2.2|1020.4|\n",
      "|2023-12-01|   3|22000.0|3.1|1609.3|8.3|-1.7|1020.7|\n",
      "|2023-12-01|   4|22000.0|3.1|1609.3|7.8|-1.7|1020.8|\n",
      "+----------+----+-------+---+------+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the weather data\n",
    "wdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|Event ID|Start Date|Start Hour|  End Date|End Hour|   Event Type|Event Borough|      Event Location|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|  684438|2023-12-09|         7|2023-12-09|      10|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  693693|2023-12-03|        12|2023-12-03|      14|Special Event|    Manhattan|Central Park: Wag...|\n",
      "|  686564|2023-12-03|        17|2023-12-03|      18|Special Event|    Manhattan|Carl Schurz Park:...|\n",
      "|  684416|2023-12-09|         9|2023-12-09|      11|Special Event|    Manhattan|Washington Square...|\n",
      "|  687023|2023-12-01|        11|2023-12-01|      12|Special Event|    Manhattan|Central Park: Lad...|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the event data\n",
    "edf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tlc aggregation for number of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup_date and pickup_hour to a timestamp and create Time column\n",
    "tdf = tdf.withColumn(\n",
    "    'Time', \n",
    "    to_timestamp(concat_ws(' ', col('pickup_date'), col('pickup_hour')))\n",
    ")\n",
    "\n",
    "# Aggregate hourly trip counts\n",
    "hourly_trip_counts = tdf.groupBy('Time').agg(count('*').alias('hourly_trip_count'))\n",
    "\n",
    "# Aggregate daily trip counts\n",
    "daily_trip_counts = tdf.groupBy('pickup_date').agg(count('*').alias('daily_trip_count'))\n",
    "\n",
    "# Join the aggregated values back to tdf\n",
    "tdf = tdf.join(hourly_trip_counts, on='Time', how='left')\n",
    "tdf = tdf.join(daily_trip_counts, on='pickup_date', how='left')\n",
    "\n",
    "# Drop the Time column if you no longer need it\n",
    "tdf = tdf.drop('Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map taxi zones to boroughs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the zones DataFrame\n",
    "zones = spark.read.csv(\"../data/landing/external/taxi_zones.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tdf DataFrame with zones to get the borough for PULocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'PUBorough'),\n",
    "               tdf['PULocationID'] == zones['LocationID'], 'left').drop('LocationID')\n",
    "\n",
    "# Join the tdf DataFrame with zones to get the borough for DOLocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'DOBorough'),\n",
    "               tdf['DOLocationID'] == zones['LocationID'], 'left').drop('LocationID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Start Date' and 'Start Hour' to create a 'Start Time' column (formatted correctly)\n",
    "edf = edf.withColumn('Start Time', concat_ws(' ', col('Start Date'), col('Start Hour')))\n",
    "\n",
    "# Aggregate the number of events per date, hour, and borough\n",
    "edf = edf.groupBy('Start Date', 'Start Hour', 'Event Borough').agg(count('Event ID').alias('Number of Events'))\n",
    "\n",
    "# Sort the results if needed\n",
    "edf = edf.orderBy('Start Date', 'Start Hour', 'Event Borough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create unified datetime columns in each dataset\n",
    "\n",
    "# For tdf (taxi dataset)\n",
    "tdf = tdf.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"pickup_date\"), col(\"pickup_hour\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# For edf (event dataset)\n",
    "edf = edf.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"Start Date\"), col(\"Start Hour\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# For wdf (weather dataset)\n",
    "wdf = wdf.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"DATE\"), col(\"HOUR\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# Step 2: Join tdf (taxi dataset) with edf (event dataset) on datetime and borough\n",
    "tdf_edf = tdf.join(edf, \n",
    "                   (tdf[\"datetime\"] == edf[\"datetime\"]) & \n",
    "                   (tdf[\"PUBorough\"] == edf[\"Event Borough\"]), \n",
    "                   \"left\")\n",
    "\n",
    "# Drop the redundant datetime column from edf after the join to avoid ambiguity\n",
    "tdf_edf = tdf_edf.drop(edf[\"datetime\"])\n",
    "\n",
    "# Step 3: Join the result with the weather dataset on datetime only (since weather data is from one station)\n",
    "final_df = tdf_edf.join(wdf, \n",
    "                        tdf_edf[\"datetime\"] == wdf[\"datetime\"], \n",
    "                        \"left\")\n",
    "\n",
    "# Step 4: Drop the redundant datetime columns after the final join\n",
    "final_df = final_df.drop(wdf[\"datetime\"]).drop(edf[\"datetime\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the redundant datetime columns after the final join \n",
    "final_df = final_df.drop(\"weather_datetime\", 'event_datetime', 'datetime', 'DATE', 'HOUR', 'Start Date', 'Start Hour', 'Event Borough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- dropoff_date: date (nullable = true)\n",
      " |-- hourly_trip_count: long (nullable = true)\n",
      " |-- daily_trip_count: long (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- DOBorough: string (nullable = true)\n",
      " |-- Number of Events: long (nullable = true)\n",
      " |-- CIG: double (nullable = true)\n",
      " |-- WND: double (nullable = true)\n",
      " |-- VIS: double (nullable = true)\n",
      " |-- TMP: double (nullable = true)\n",
      " |-- DEW: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the final dataframe\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.select(\n",
    "    'pickup_date', 'pickup_hour', 'dropoff_date', 'dropoff_hour',\n",
    "    'VendorID', 'passenger_count', 'trip_distance', 'trip_duration',\n",
    "    'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'PUBorough',\n",
    "    'DOLocationID', 'DOBorough', 'payment_type',\n",
    "    'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "    'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'ehail_fee',\n",
    "    'hourly_trip_count', 'daily_trip_count',\n",
    "    'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP',\n",
    "    'Number of Events'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "The assumption is no events occured if number of evnets in Null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in Number of Events column with 0\n",
    "final_df = final_df.fillna(0, subset=['Number of Events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode for PUBorough based on PULocationID\n",
    "puborough_mode = final_df.groupBy(\"PULocationID\", \"PUBorough\").agg(count(\"*\").alias(\"count\"))\n",
    "puborough_mode = puborough_mode.withColumn(\"row\", row_number().over(Window.partitionBy(\"PULocationID\").orderBy(col(\"count\").desc())))\n",
    "puborough_mode = puborough_mode.filter(col(\"row\") == 1).select(\"PULocationID\", col(\"PUBorough\").alias(\"PUBorough_mode\"))\n",
    "\n",
    "# Join to fill in missing PUBorough\n",
    "final_df = final_df.join(puborough_mode, \"PULocationID\", \"left\").withColumn(\n",
    "    \"PUBorough\", coalesce(col(\"PUBorough\"), col(\"PUBorough_mode\"))\n",
    ").drop(\"PUBorough_mode\")\n",
    "\n",
    "# Calculate the mode for DOBorough based on DOLocationID\n",
    "doborough_mode = final_df.groupBy(\"DOLocationID\", \"DOBorough\").agg(count(\"*\").alias(\"count\"))\n",
    "doborough_mode = doborough_mode.withColumn(\"row\", row_number().over(Window.partitionBy(\"DOLocationID\").orderBy(col(\"count\").desc())))\n",
    "doborough_mode = doborough_mode.filter(col(\"row\") == 1).select(\"DOLocationID\", col(\"DOBorough\").alias(\"DOBorough_mode\"))\n",
    "\n",
    "# Join to fill in missing DOBorough\n",
    "final_df = final_df.join(doborough_mode, \"DOLocationID\", \"left\").withColumn(\n",
    "    \"DOBorough\", coalesce(col(\"DOBorough\"), col(\"DOBorough_mode\"))\n",
    ").drop(\"DOBorough_mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification with proper partitioning\n",
    "window_spec = Window.partitionBy(\"pickup_date\").orderBy(\"pickup_hour\").rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "# Apply forward fill to the missing weather columns\n",
    "final_df = final_df.withColumn(\"CIG\", last(col(\"CIG\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"WND\", last(col(\"WND\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"VIS\", last(col(\"VIS\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"TMP\", last(col(\"TMP\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"DEW\", last(col(\"DEW\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"SLP\", last(col(\"SLP\"), ignorenulls=True).over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows within the date range of the taxi data\n",
    "final_df = final_df.filter(col(\"pickup_date\").between(\"2023-12-01\", \"2024-5-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export file to curated folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the merged data\n",
    "final_df.write.parquet(\"../data/curated/tlc_data/first_cleaned.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
