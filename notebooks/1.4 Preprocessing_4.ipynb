{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing_final_1\n",
    "This notebook merges 3 preprocessed datasets and prepares for EDA_hourly and model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from preprocessed tlc data\n",
    "tdf = spark.read.parquet(\"../data/raw/tlc_df.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed weather data\n",
    "wdf = spark.read.csv(\"../data/raw/NYC_weather_raw.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed event data\n",
    "edf = spark.read.parquet(\"../data/raw/NYC_Permitted_Event_Information_Historical.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|pickup_hour|dropoff_hour|pickup_date|dropoff_date|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|       1|              1|          7.1|         1|             false|         249|         179|           1|       32.4|  3.5|    0.5|      7.45|         0.0|                  1.0|       44.85|                 2.5|      0.0|         32.0|          0|           1| 2023-07-01|  2023-07-01|\n",
      "|       2|              2|         18.9|         2|             false|         132|         230|           1|       70.0|  0.0|    0.5|     16.11|        6.55|                  1.0|       98.41|                 2.5|      0.0|         48.0|          0|           1| 2023-07-01|  2023-07-01|\n",
      "|       2|              1|         1.24|         1|             false|         164|         230|           2|        9.3|  1.0|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|      0.0|          8.0|          0|           0| 2023-07-01|  2023-07-01|\n",
      "|       1|              2|         14.8|         1|             false|         132|         131|           2|       54.8| 2.75|    0.5|       0.0|         0.0|                  1.0|       59.05|                 0.0|      0.0|         21.0|          0|           0| 2023-07-01|  2023-07-01|\n",
      "|       2|              1|          9.6|         1|             false|         144|         198|           1|       40.8|  1.0|    0.5|      9.16|         0.0|                  1.0|       54.96|                 2.5|      0.0|         28.0|          0|           0| 2023-07-01|  2023-07-01|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the tlc data\n",
    "tdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- dropoff_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema of the tlc data\n",
    "tdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+------------------+-----+----+----+------+\n",
      "|      DATE|HOUR|    CIG|               WND|  VIS| TMP| DEW|   SLP|\n",
      "+----------+----+-------+------------------+-----+----+----+------+\n",
      "|2023-07-01|   0|22000.0|2.6319672131147542|965.6|23.9|13.3|1017.1|\n",
      "|2023-07-01|   1|22000.0|2.6319672131147542|965.6|23.3|13.3|1017.6|\n",
      "|2023-07-01|   2|22000.0|2.6319672131147542|965.6|23.3|12.8|1017.8|\n",
      "|2023-07-01|   3|22000.0|               3.1|965.6|22.8|12.8|1017.7|\n",
      "|2023-07-01|   4|22000.0|               1.5|965.6|22.8|11.7|1017.4|\n",
      "+----------+----+-------+------------------+-----+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the weather data\n",
    "wdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|Event ID|Start Date|Start Hour|  End Date|End Hour|   Event Type|Event Borough|      Event Location|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|  683185|2023-11-17|         8|2023-11-17|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  683046|2023-10-13|         8|2023-10-13|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  682144|2023-07-14|         8|2023-07-14|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  681104|2023-09-17|        15|2023-09-17|      20|Special Event|     Brooklyn|  Fulton Park: Plaza|\n",
      "|  683192|2023-11-25|         8|2023-11-25|      23|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the event data\n",
    "edf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|min(Start Date)|max(Start Date)|\n",
      "+---------------+---------------+\n",
      "|     2023-07-01|     2023-12-30|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check min and max date for edf\n",
    "edf.select(F.min(\"Start Date\"), F.max(\"Start Date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map taxi zones to boroughs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the zones DataFrame\n",
    "zones = spark.read.csv(\"../data/landing/external/taxi_zones.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tdf DataFrame with zones to get the borough for PULocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'PUBorough'),\n",
    "               tdf['PULocationID'] == zones['LocationID'], 'left').drop('LocationID')\n",
    "\n",
    "# Join the tdf DataFrame with zones to get the borough for DOLocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'DOBorough'),\n",
    "               tdf['DOLocationID'] == zones['LocationID'], 'left').drop('LocationID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup_date and pickup_hour to a timestamp and create Time column\n",
    "tdf = tdf.withColumn(\n",
    "    'Time', \n",
    "    to_timestamp(concat_ws(' ', col('pickup_date'), col('pickup_hour')))\n",
    ")\n",
    "\n",
    "df_hourly_agg = tdf.groupBy(['pickup_date', 'pickup_hour', \"PUBorough\"]).agg({\n",
    "    '*': 'count',\n",
    "}).withColumnRenamed('count(1)', 'hourly_trip_count')\n",
    "df_daily_agg = tdf.groupBy(['pickup_date', \"PUBorough\"]).agg({\n",
    "    '*': 'count',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the number of events per date, hour, and borough\n",
    "edf_hourly = edf.groupBy('Start Date', 'Start Hour', 'Event Borough').agg(count('Event ID').alias('Number of Events'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+----------------+\n",
      "|Start Date|Start Hour|Event Borough|Number of Events|\n",
      "+----------+----------+-------------+----------------+\n",
      "|2023-08-14|        10|    Manhattan|              41|\n",
      "|2023-08-19|         8|       Queens|              87|\n",
      "|2023-07-28|         8|        Bronx|              46|\n",
      "|2023-07-09|        10|        Bronx|              33|\n",
      "|2023-07-29|        14|    Manhattan|              26|\n",
      "+----------+----------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edf_hourly.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create unified datetime columns in each dataset\n",
    "\n",
    "# For tdf (taxi dataset)\n",
    "df_hourly_agg = df_hourly_agg.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"pickup_date\"), col(\"pickup_hour\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# For edf (event dataset)\n",
    "edf_hourly = edf_hourly.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"Start Date\"), col(\"Start Hour\")), \"yyyy-MM-dd H\"))\n",
    "edf_hourly = edf_hourly.withColumnRenamed(\"Event Borough\", \"PUBorough\")\n",
    "\n",
    "# For wdf (weather dataset)\n",
    "wdf = wdf.withColumn(\"datetime\", to_timestamp(col(\"DATE\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Step 2: Join tdf (taxi dataset) with edf (event dataset) on datetime and borough\n",
    "tdf_edf = df_hourly_agg.join(edf_hourly, on=[\"datetime\", \"PUBorough\"], how=\"left\")\n",
    "\n",
    "# Step 3: Join the result with the weather dataset on datetime only (since weather data is from one station)\n",
    "final_df = tdf_edf.join(wdf, \n",
    "                        on=\"datetime\", \n",
    "                        how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- hourly_trip_count: long (nullable = false)\n",
      " |-- Start Date: date (nullable = true)\n",
      " |-- Start Hour: integer (nullable = true)\n",
      " |-- Number of Events: long (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- HOUR: integer (nullable = true)\n",
      " |-- CIG: double (nullable = true)\n",
      " |-- WND: double (nullable = true)\n",
      " |-- VIS: double (nullable = true)\n",
      " |-- TMP: double (nullable = true)\n",
      " |-- DEW: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the final dataframe\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the redundant datetime columns after the final join \n",
    "final_df = final_df.drop('Start Date', 'Start Hour', 'DATE', 'HOUR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "The assumption is no events occured if number of event in Null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in Number of Events column with 0\n",
    "final_df = final_df.fillna(0, subset=['Number of Events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification with proper partitioning\n",
    "window_spec = Window.partitionBy(\"pickup_date\").orderBy(\"pickup_hour\").rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "# Apply forward fill to the missing weather columns\n",
    "final_df = final_df.withColumn(\"CIG\", last(col(\"CIG\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"WND\", last(col(\"WND\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"VIS\", last(col(\"VIS\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"TMP\", last(col(\"TMP\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"DEW\", last(col(\"DEW\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"SLP\", last(col(\"SLP\"), ignorenulls=True).over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling error values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|min(pickup_date)|max(pickup_date)|\n",
      "+----------------+----------------+\n",
      "|      2023-06-30|      2024-01-03|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.select(F.min('pickup_date'), F.max('pickup_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows within the date range of the taxi data\n",
    "final_df = final_df.filter(col(\"pickup_date\").between(\"2023-07-01\", \"2023-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|min(pickup_date)|max(pickup_date)|\n",
      "+----------------+----------------+\n",
      "|      2023-07-01|      2023-12-31|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check date range\n",
    "final_df.select(F.min('pickup_date'), F.max('pickup_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- hourly_trip_count: long (nullable = false)\n",
      " |-- Number of Events: long (nullable = true)\n",
      " |-- CIG: double (nullable = true)\n",
      " |-- WND: double (nullable = true)\n",
      " |-- VIS: double (nullable = true)\n",
      " |-- TMP: double (nullable = true)\n",
      " |-- DEW: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export file to curated folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the merged data and overwrite the previous one\n",
    "final_df.write.parquet(\"../data/curated/tlc_data/first_cleaned.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
