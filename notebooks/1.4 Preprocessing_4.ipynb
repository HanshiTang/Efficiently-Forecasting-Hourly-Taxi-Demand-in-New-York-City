{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing_final\n",
    "This notebook aggregates 3 preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from preprocessed tlc data\n",
    "tdf = spark.read.parquet(\"../data/raw/tlc_df.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed weather data\n",
    "wdf = spark.read.csv(\"../data/raw/NYC_weather_raw.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed event data\n",
    "edf = spark.read.parquet(\"../data/raw/NYC_Permitted_Event_Information_Historical.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|pickup_hour|dropoff_hour|pickup_date|dropoff_date|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|       2|              1|         1.75|         1|             false|          68|          50|           2|       12.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        17.1|                 2.5|      0.0|         10.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       1|              1|          3.6|         1|             false|          48|          43|           1|       17.7|  3.5|    0.5|      4.55|         0.0|                  1.0|       27.25|                 2.5|      0.0|         16.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|        10.28|         1|             false|         170|          95|           1|       44.3|  1.0|    0.5|     11.25|        6.94|                  1.0|       67.49|                 2.5|      0.0|         29.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|         2.66|         1|             false|         163|         158|           1|       14.9|  1.0|    0.5|      3.98|         0.0|                  1.0|       23.88|                 2.5|      0.0|         14.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|        15.43|         1|             false|         132|         150|           2|       59.0|  1.0|    0.5|       0.0|         0.0|                  1.0|       63.25|                 0.0|      0.0|         24.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the tlc data\n",
    "tdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- dropoff_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema of the tlc data\n",
    "tdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+---+------+---+----+------+\n",
      "|      DATE|HOUR|    CIG|WND|   VIS|TMP| DEW|   SLP|\n",
      "+----------+----+-------+---+------+---+----+------+\n",
      "|2023-12-01|   0|22000.0|4.1|1609.3|9.4|-2.8|1020.1|\n",
      "|2023-12-01|   1|22000.0|2.1|1609.3|8.9|-2.2|1020.0|\n",
      "|2023-12-01|   2|22000.0|3.1|1609.3|8.9|-2.2|1020.4|\n",
      "|2023-12-01|   3|22000.0|3.1|1609.3|8.3|-1.7|1020.7|\n",
      "|2023-12-01|   4|22000.0|3.1|1609.3|7.8|-1.7|1020.8|\n",
      "+----------+----+-------+---+------+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the weather data\n",
    "wdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|Event ID|Start Date|Start Hour|  End Date|End Hour|   Event Type|Event Borough|      Event Location|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "|  684438|2023-12-09|         7|2023-12-09|      10|Special Event|     Brooklyn|Prospect Park: Pi...|\n",
      "|  693693|2023-12-03|        12|2023-12-03|      14|Special Event|    Manhattan|Central Park: Wag...|\n",
      "|  686564|2023-12-03|        17|2023-12-03|      18|Special Event|    Manhattan|Carl Schurz Park:...|\n",
      "|  684416|2023-12-09|         9|2023-12-09|      11|Special Event|    Manhattan|Washington Square...|\n",
      "|  687023|2023-12-01|        11|2023-12-01|      12|Special Event|    Manhattan|Central Park: Lad...|\n",
      "+--------+----------+----------+----------+--------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 5 rows of the event data\n",
    "edf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tlc aggregation for number of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup_date and pickup_hour to a timestamp and create Time column\n",
    "tdf = tdf.withColumn(\n",
    "    'Time', \n",
    "    to_timestamp(concat_ws(' ', col('pickup_date'), col('pickup_hour')))\n",
    ")\n",
    "\n",
    "# Aggregate hourly trip counts\n",
    "hourly_trip_counts = tdf.groupBy('Time').agg(count('*').alias('hourly_trip_count'))\n",
    "\n",
    "# Aggregate daily trip counts\n",
    "daily_trip_counts = tdf.groupBy('pickup_date').agg(count('*').alias('daily_trip_count'))\n",
    "\n",
    "# drop the Time column\n",
    "tdf = tdf.drop('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|pickup_hour|dropoff_hour|pickup_date|dropoff_date|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "|       2|              1|         1.75|         1|             false|          68|          50|           2|       12.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        17.1|                 2.5|      0.0|         10.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       1|              1|          3.6|         1|             false|          48|          43|           1|       17.7|  3.5|    0.5|      4.55|         0.0|                  1.0|       27.25|                 2.5|      0.0|         16.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|        10.28|         1|             false|         170|          95|           1|       44.3|  1.0|    0.5|     11.25|        6.94|                  1.0|       67.49|                 2.5|      0.0|         29.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|         2.66|         1|             false|         163|         158|           1|       14.9|  1.0|    0.5|      3.98|         0.0|                  1.0|       23.88|                 2.5|      0.0|         14.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "|       2|              1|        15.43|         1|             false|         132|         150|           2|       59.0|  1.0|    0.5|       0.0|         0.0|                  1.0|       63.25|                 0.0|      0.0|         24.0|          0|           0| 2023-12-01|  2023-12-01|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map taxi zones to boroughs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------------+----------------+--------------------+----------+-------------+\n",
      "|OBJECTID|     Shape_Leng|            the_geom|      Shape_Area|                zone|LocationID|      borough|\n",
      "+--------+---------------+--------------------+----------------+--------------------+----------+-------------+\n",
      "|       1| 0.116357453189|MULTIPOLYGON (((-...|  7.823067885E-4|      Newark Airport|         1|          EWR|\n",
      "|       2|  0.43346966679|MULTIPOLYGON (((-...|0.00486634037837|         Jamaica Bay|         2|       Queens|\n",
      "|       3|0.0843411059012|MULTIPOLYGON (((-...|3.14414156821E-4|Allerton/Pelham G...|         3|        Bronx|\n",
      "|       4|0.0435665270921|MULTIPOLYGON (((-...|1.11871946192E-4|       Alphabet City|         4|    Manhattan|\n",
      "|       5|0.0921464898574|MULTIPOLYGON (((-...|4.97957489363E-4|       Arden Heights|         5|Staten Island|\n",
      "+--------+---------------+--------------------+----------------+--------------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the zones DataFrame\n",
    "zones = spark.read.csv(\"../data/landing/external/taxi_zones.csv\", header=True, inferSchema=True)\n",
    "# show 5 rows of the zones data\n",
    "zones.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tdf DataFrame with zones to get the borough for PULocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'PUBorough'),\n",
    "               tdf['PULocationID'] == zones['LocationID'], 'left').drop('LocationID')\n",
    "\n",
    "# Join the tdf DataFrame with zones to get the borough for DOLocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'DOBorough'),\n",
    "               tdf['DOLocationID'] == zones['LocationID'], 'left').drop('LocationID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+---------+---------+\n",
      "|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|pickup_hour|dropoff_hour|pickup_date|dropoff_date|PUBorough|DOBorough|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+---------+---------+\n",
      "|       2|              1|         1.75|         1|             false|          68|          50|           2|       12.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        17.1|                 2.5|      0.0|         10.0|          0|           0| 2023-12-01|  2023-12-01|Manhattan|Manhattan|\n",
      "|       1|              1|          3.6|         1|             false|          48|          43|           1|       17.7|  3.5|    0.5|      4.55|         0.0|                  1.0|       27.25|                 2.5|      0.0|         16.0|          0|           0| 2023-12-01|  2023-12-01|Manhattan|Manhattan|\n",
      "|       2|              1|        10.28|         1|             false|         170|          95|           1|       44.3|  1.0|    0.5|     11.25|        6.94|                  1.0|       67.49|                 2.5|      0.0|         29.0|          0|           0| 2023-12-01|  2023-12-01|Manhattan|   Queens|\n",
      "|       2|              1|         2.66|         1|             false|         163|         158|           1|       14.9|  1.0|    0.5|      3.98|         0.0|                  1.0|       23.88|                 2.5|      0.0|         14.0|          0|           0| 2023-12-01|  2023-12-01|Manhattan|Manhattan|\n",
      "|       2|              1|        15.43|         1|             false|         132|         150|           2|       59.0|  1.0|    0.5|       0.0|         0.0|                  1.0|       63.25|                 0.0|      0.0|         24.0|          0|           0| 2023-12-01|  2023-12-01|   Queens| Brooklyn|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+-----------+------------+-----------+------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+----------------+\n",
      "|Start Date|Start Hour|Event Borough|Number of Events|\n",
      "+----------+----------+-------------+----------------+\n",
      "|2023-12-01|         0|    Manhattan|              36|\n",
      "|2023-12-01|         1|     Brooklyn|               1|\n",
      "|2023-12-01|         7|    Manhattan|               2|\n",
      "|2023-12-01|         7|       Queens|               1|\n",
      "|2023-12-01|         8|        Bronx|               7|\n",
      "|2023-12-01|         8|     Brooklyn|              10|\n",
      "|2023-12-01|         8|    Manhattan|              12|\n",
      "|2023-12-01|         8|       Queens|               6|\n",
      "|2023-12-01|         8|Staten Island|               1|\n",
      "|2023-12-01|         9|        Bronx|               2|\n",
      "|2023-12-01|         9|     Brooklyn|               4|\n",
      "|2023-12-01|         9|    Manhattan|               5|\n",
      "|2023-12-01|         9|       Queens|               3|\n",
      "|2023-12-01|         9|Staten Island|               4|\n",
      "|2023-12-01|        10|        Bronx|               2|\n",
      "|2023-12-01|        10|     Brooklyn|               3|\n",
      "|2023-12-01|        10|    Manhattan|               4|\n",
      "|2023-12-01|        10|       Queens|               5|\n",
      "|2023-12-01|        11|     Brooklyn|               1|\n",
      "|2023-12-01|        11|    Manhattan|               6|\n",
      "+----------+----------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine 'Start Date' and 'Start Hour' to create a 'Start Time' column (formatted correctly)\n",
    "edf = edf.withColumn('Start Time', concat_ws(' ', col('Start Date'), col('Start Hour')))\n",
    "\n",
    "# Aggregate the number of events per date, hour, and borough\n",
    "edf = edf.groupBy('Start Date', 'Start Hour', 'Event Borough').agg(count('Event ID').alias('Number of Events'))\n",
    "\n",
    "# Sort the results if needed\n",
    "edf = edf.orderBy('Start Date', 'Start Hour', 'Event Borough')\n",
    "\n",
    "# Show or save the aggregated results\n",
    "edf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create unified datetime columns in each dataset\n",
    "\n",
    "# For tdf (taxi dataset)\n",
    "tdf = tdf.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"pickup_date\"), col(\"pickup_hour\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# For edf (event dataset)\n",
    "edf = edf.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"Start Date\"), col(\"Start Hour\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# For wdf (weather dataset)\n",
    "wdf = wdf.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"DATE\"), col(\"HOUR\")), \"yyyy-MM-dd H\"))\n",
    "\n",
    "# Step 2: Join tdf (taxi dataset) with edf (event dataset) on datetime and borough\n",
    "tdf_edf = tdf.join(edf, \n",
    "                   (tdf[\"datetime\"] == edf[\"datetime\"]) & \n",
    "                   (tdf[\"PUBorough\"] == edf[\"Event Borough\"]), \n",
    "                   \"left\")\n",
    "\n",
    "# Drop the redundant datetime column from edf after the join to avoid ambiguity\n",
    "tdf_edf = tdf_edf.drop(edf[\"datetime\"])\n",
    "\n",
    "# Step 3: Join the result with the weather dataset on datetime only (since weather data is from one station)\n",
    "final_df = tdf_edf.join(wdf, \n",
    "                        tdf_edf[\"datetime\"] == wdf[\"datetime\"], \n",
    "                        \"left\")\n",
    "\n",
    "# Step 4: Drop the redundant datetime columns after the final join\n",
    "final_df = final_df.drop(wdf[\"datetime\"]).drop(edf[\"datetime\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the redundant datetime columns after the final join \n",
    "final_df = final_df.drop(\"weather_datetime\", 'event_datetime', 'datetime', 'DATE', 'HOUR', 'Start Date', 'Start Hour', 'Event Borough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.select(\n",
    "    \"pickup_date\", \"pickup_hour\", \"dropoff_date\", \"dropoff_hour\",  # Date and Time Columns\n",
    "    \"VendorID\", \"passenger_count\", \"trip_distance\", \"RatecodeID\",  # Trip Information\n",
    "    \"PULocationID\", \"PUBorough\", \"DOLocationID\", \"DOBorough\",      # Location Information\n",
    "    \"payment_type\", \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",  # Payment Details\n",
    "    \"congestion_surcharge\", \"total_amount\", \"ehail_fee\",            # Additional Charges\n",
    "    \"trip_duration\",                                                 # Duration\n",
    "    \"Number of Events\",                                              # Event Data\n",
    "    \"CIG\", \"WND\", \"VIS\", \"TMP\", \"DEW\", \"SLP\"                        # Weather Data\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "The assumption is no events occured if number of evnets in Null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in Number of Events column with 0\n",
    "final_df = final_df.fillna(0, subset=['Number of Events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode for PUBorough based on PULocationID\n",
    "puborough_mode = final_df.groupBy(\"PULocationID\", \"PUBorough\").agg(count(\"*\").alias(\"count\"))\n",
    "puborough_mode = puborough_mode.withColumn(\"row\", row_number().over(Window.partitionBy(\"PULocationID\").orderBy(col(\"count\").desc())))\n",
    "puborough_mode = puborough_mode.filter(col(\"row\") == 1).select(\"PULocationID\", col(\"PUBorough\").alias(\"PUBorough_mode\"))\n",
    "\n",
    "# Join to fill in missing PUBorough\n",
    "final_df = final_df.join(puborough_mode, \"PULocationID\", \"left\").withColumn(\n",
    "    \"PUBorough\", coalesce(col(\"PUBorough\"), col(\"PUBorough_mode\"))\n",
    ").drop(\"PUBorough_mode\")\n",
    "\n",
    "# Calculate the mode for DOBorough based on DOLocationID\n",
    "doborough_mode = final_df.groupBy(\"DOLocationID\", \"DOBorough\").agg(count(\"*\").alias(\"count\"))\n",
    "doborough_mode = doborough_mode.withColumn(\"row\", row_number().over(Window.partitionBy(\"DOLocationID\").orderBy(col(\"count\").desc())))\n",
    "doborough_mode = doborough_mode.filter(col(\"row\") == 1).select(\"DOLocationID\", col(\"DOBorough\").alias(\"DOBorough_mode\"))\n",
    "\n",
    "# Join to fill in missing DOBorough\n",
    "final_df = final_df.join(doborough_mode, \"DOLocationID\", \"left\").withColumn(\n",
    "    \"DOBorough\", coalesce(col(\"DOBorough\"), col(\"DOBorough_mode\"))\n",
    ").drop(\"DOBorough_mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification to forward fill missing values\n",
    "window_spec = Window.partitionBy().orderBy(\"pickup_date\", \"pickup_hour\").rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "# Apply forward fill to the missing weather columns\n",
    "final_df = final_df.withColumn(\"CIG\", last(col(\"CIG\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"WND\", last(col(\"WND\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"VIS\", last(col(\"VIS\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"TMP\", last(col(\"TMP\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"DEW\", last(col(\"DEW\"), ignorenulls=True).over(window_spec))\n",
    "final_df = final_df.withColumn(\"SLP\", last(col(\"SLP\"), ignorenulls=True).over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows within the date range of the taxi data\n",
    "final_df = final_df.filter(col(\"pickup_date\").between(\"2023-12-01\", \"2020-5-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:23:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:23:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:24:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 03:24:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 140:>  (0 + 8) / 8][Stage 142:>  (0 + 0) / 1][Stage 143:>  (0 + 0) / 1]8]\r"
     ]
    }
   ],
   "source": [
    "# check missing values for each column\n",
    "# Calculate the number of missing (null) values for each column\n",
    "missing_values = final_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in final_df.columns])\n",
    "\n",
    "# Show the result\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export file to curated folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the merged data\n",
    "final_df.write.parquet(\"../data/curated/tlc_data/first_cleaned.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
