{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing TLC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans the following datasets: \n",
    "1. Yellow taxi data from 2023-06 to 2024-05\n",
    "2. Green taxi data from 2023-06 to 2024-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-2024 TLC data\n",
    "df = spark.read.parquet('../data/landing/tlc_data/*.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-6 to 2024-5 yellow data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "ydf_2023_6 = spark.read.parquet(path + \"/Y-2023-06.parquet\")\n",
    "ydf_2023_7 = spark.read.parquet(path + \"/Y-2023-07.parquet\")\n",
    "ydf_2023_8 = spark.read.parquet(path + \"/Y-2023-08.parquet\")\n",
    "ydf_2023_9 = spark.read.parquet(path + \"/Y-2023-09.parquet\")\n",
    "ydf_2023_10 = spark.read.parquet(path + \"/Y-2023-10.parquet\")\n",
    "ydf_2023_11 = spark.read.parquet(path + \"/Y-2023-11.parquet\")\n",
    "ydf_2023_12 = spark.read.parquet(path + \"/Y-2023-12.parquet\")\n",
    "ydf_2024_1 = spark.read.parquet(path + \"/Y-2024-01.parquet\")\n",
    "ydf_2024_2 = spark.read.parquet(path + \"/Y-2024-02.parquet\")\n",
    "ydf_2024_3 = spark.read.parquet(path + \"/Y-2024-03.parquet\")\n",
    "ydf_2024_4 = spark.read.parquet(path + \"/Y-2024-04.parquet\")\n",
    "ydf_2024_5 = spark.read.parquet(path + \"/Y-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-6 to 2024-5 green data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "gdf_2023_6 = spark.read.parquet(path + \"/G-2023-06.parquet\")\n",
    "gdf_2023_7 = spark.read.parquet(path + \"/G-2023-07.parquet\")\n",
    "gdf_2023_8 = spark.read.parquet(path + \"/G-2023-08.parquet\")\n",
    "gdf_2023_9 = spark.read.parquet(path + \"/G-2023-09.parquet\")\n",
    "gdf_2023_10 = spark.read.parquet(path + \"/G-2023-10.parquet\")\n",
    "gdf_2023_11 = spark.read.parquet(path + \"/G-2023-11.parquet\")\n",
    "gdf_2023_12 = spark.read.parquet(path + \"/G-2023-12.parquet\")\n",
    "gdf_2024_1 = spark.read.parquet(path + \"/G-2024-01.parquet\")\n",
    "gdf_2024_2 = spark.read.parquet(path + \"/G-2024-02.parquet\")\n",
    "gdf_2024_3 = spark.read.parquet(path + \"/G-2024-03.parquet\")\n",
    "gdf_2024_4 = spark.read.parquet(path + \"/G-2024-04.parquet\")\n",
    "gdf_2024_5 = spark.read.parquet(path + \"/G-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLC datasets inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total green count is 732489.\n",
      "The total yellow count is 38916740.\n",
      "The total count is 39649229.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total row count for yellow taxi data from 2023-6 to 2024-5\n",
    "yellow_count = (\n",
    "    ydf_2023_6.count() + \n",
    "    ydf_2023_7.count() + \n",
    "    ydf_2023_8.count() + \n",
    "    ydf_2023_9.count() + \n",
    "    ydf_2023_10.count() + \n",
    "    ydf_2023_11.count() + \n",
    "    ydf_2023_12.count() + \n",
    "    ydf_2024_1.count() + \n",
    "    ydf_2024_2.count() + \n",
    "    ydf_2024_3.count() + \n",
    "    ydf_2024_4.count() + \n",
    "    ydf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Calculate the total row count for green taxi data from 2023-6 to 2024-5\n",
    "green_count = (\n",
    "    gdf_2023_6.count() + \n",
    "    gdf_2023_7.count() + \n",
    "    gdf_2023_8.count() + \n",
    "    gdf_2023_9.count() + \n",
    "    gdf_2023_10.count() + \n",
    "    gdf_2023_11.count() + \n",
    "    gdf_2023_12.count() + \n",
    "    gdf_2024_1.count() + \n",
    "    gdf_2024_2.count() + \n",
    "    gdf_2024_3.count() + \n",
    "    gdf_2024_4.count() + \n",
    "    gdf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Display the green count\n",
    "print(f\"The total green count is {green_count}.\")\n",
    "\n",
    "# Display the yellow count\n",
    "print(f\"The total yellow count is {yellow_count}.\")\n",
    "\n",
    "# Calculate the total row count for all taxi data from 2023-6 to 2024-5\n",
    "total_count = yellow_count + green_count\n",
    "# Display the total count\n",
    "print(f\"The total count is {total_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns only in yellowDF: {'Airport_fee', 'tpep_dropoff_datetime', 'tpep_pickup_datetime'}\n",
      "Columns only in greenDF: {'lpep_pickup_datetime', 'ehail_fee', 'trip_type', 'lpep_dropoff_datetime'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns of each DataFrame\n",
    "columns_ydf = set(ydf_2024_5.columns)\n",
    "columns_gdf = set(gdf_2024_5.columns)\n",
    "\n",
    "# Find differences in columns\n",
    "columns_only_in_df1 = columns_ydf - columns_gdf\n",
    "columns_only_in_df2 = columns_gdf - columns_ydf\n",
    "\n",
    "print(f\"Columns only in yellowDF: {columns_only_in_df1}\")\n",
    "print(f\"Columns only in greenDF: {columns_only_in_df2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in yellowDF: 19\n",
      "Number of features in greenDF: 20\n"
     ]
    }
   ],
   "source": [
    "# Report number of features in each DataFrame\n",
    "print(f\"Number of features in yellowDF: {len(ydf_2024_5.columns)}\")\n",
    "print(f\"Number of features in greenDF: {len(gdf_2024_5.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets \n",
    "ydfs = [ydf_2023_6, ydf_2023_7, ydf_2023_8, ydf_2023_9, ydf_2023_10, ydf_2023_11, ydf_2023_12, \n",
    "       ydf_2024_1, ydf_2024_2, ydf_2024_3, ydf_2024_4, ydf_2024_5]\n",
    "gdfs = [gdf_2023_6, gdf_2023_7, gdf_2023_8, gdf_2023_9, gdf_2023_10, gdf_2023_11, gdf_2023_12, \n",
    "       gdf_2024_1, gdf_2024_2, gdf_2024_3, gdf_2024_4, gdf_2024_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "# Function to union two DataFrames\n",
    "def union_dfs(df1, df2):\n",
    "    return df1.unionByName(df2)\n",
    "\n",
    "# Combine all yellow taxi data\n",
    "yellow_combined = reduce(union_dfs, ydfs)\n",
    "\n",
    "# Combine all green taxi data\n",
    "green_combined = reduce(union_dfs, gdfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-----------------+----------------+---------------------+------------------+--------------------+-------------------+\n",
      "|summary|          VendorID|   passenger_count|    trip_distance|       RatecodeID|store_and_fwd_flag|      PULocationID|     DOLocationID|      payment_type|       fare_amount|             extra|            mta_tax|       tip_amount|    tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|        Airport_fee|\n",
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-----------------+----------------+---------------------+------------------+--------------------+-------------------+\n",
      "|  count|          38916740|          36470845|         38916740|         36470845|          36470845|          38916740|         38916740|          38916740|          38916740|          38916740|           38916740|         38916740|        38916740|             38916740|          38916740|            36470845|           36470845|\n",
      "|   mean|1.7501221325321699| 1.358100422405897|4.335654643734171|1.916763760203527|              NULL|164.85808767640867|163.8784072869413|1.1486949317954176|19.423297645693193| 1.483277365216099|0.48366842340853844|3.432825563241012|0.58191261061437|   0.9756503782176966|28.233441648772605|  2.2551156341455756|0.15013224810118878|\n",
      "| stddev|0.4364434190351839|0.8686641826544039|291.0124916327774| 9.02114153144218|              NULL| 64.08273002484995|69.69262070009012|0.5963151527061763| 92.67011115103949|2.4387757316601157|0.11672837920257821|4.148562268712141|2.23941114867875|  0.21568366505028796| 93.83428471852012|  0.8222473575330497| 0.5009239971086095|\n",
      "|    min|                 1|                 0|              0.0|                1|                 N|                 1|                1|                 0|           -1087.3|            -39.17|               -0.5|          -330.88|           -91.3|                 -1.0|          -1094.05|                -2.5|              -1.75|\n",
      "|    max|                 6|                 9|        345729.44|               99|                 Y|               265|              265|                 5|         386983.63|           10002.5|              52.09|           4174.0|         1702.88|                  1.0|         386987.63|                2.75|               1.75|\n",
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-----------------+----------------+---------------------+------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1128:====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+---------+---------------------+------------------+------------------+-------------------+--------------------+\n",
      "|summary|          VendorID|store_and_fwd_flag|       RatecodeID|     PULocationID|     DOLocationID|   passenger_count|     trip_distance|      fare_amount|             extra|            mta_tax|        tip_amount|      tolls_amount|ehail_fee|improvement_surcharge|      total_amount|      payment_type|          trip_type|congestion_surcharge|\n",
      "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+---------+---------------------+------------------+------------------+-------------------+--------------------+\n",
      "|  count|            732489|            687452|           687452|           732489|           732489|            687452|            732489|           732489|            732489|             732489|            732489|            732489|        0|               732489|            732489|            687452|             687398|              687452|\n",
      "|   mean|1.8743080100861582|              NULL|1.184039031088716|97.20224194493024|140.0183415723649|1.3024560260207259|21.278328807668085|18.51681496923479|0.8998953567903408| 0.5678959684036211|2.4147240299855293|0.2501005202808494|     NULL|   0.9853301551286152|24.195434197646332| 1.330407068420777| 1.0393731142656801|  0.7664738774488983|\n",
      "| stddev|0.3315021321395578|              NULL|1.187830955858815|58.66257523361039|76.37922991998644| 0.947010667407842|1096.4845122711827|19.18914000738103|1.3702476448019112|0.37912600001386465| 3.329785423272552|1.3750869592947366|     NULL|   0.1389113535599435| 20.99632089533045|0.5015365398338647|0.19448117431266207|   1.232505647577715|\n",
      "|    min|                 1|                 N|                1|                1|                1|                 0|               0.0|           -500.0|              -6.0|               -0.5|             -65.0|             -6.94|     NULL|                 -1.0|            -501.0|                 1|                  1|               -2.75|\n",
      "|    max|                 2|                 Y|               99|              265|              265|                 9|         278990.28|           4003.0|              12.0|               4.25|            343.24|              51.0|     NULL|                  1.0|            4004.5|                 5|                  2|                2.75|\n",
      "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+---------+---------------------+------------------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show descriptive statistics for taxi data\n",
    "yellow_combined.describe().show()\n",
    "green_combined.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify the columns of the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Airport_fee and trip_type columns from the combined data\n",
    "yellow_combined = yellow_combined.drop(\"Airport_fee\")\n",
    "green_combined = green_combined.drop(\"trip_type\")\n",
    "\n",
    "# Set ehail_fee to 0 for yellow taxi data\n",
    "yellow_combined = yellow_combined.withColumn(\"ehail_fee\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tpep: Taxicab Passenger Enhancement Program for yellow taxi <br> \n",
    "lpep: Livery Passenger Enhancement Program for green taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename datetime columns to be consistent\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "yellow_combined = yellow_combined.withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                               .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "green_combined = green_combined.withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                             .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge yellow and green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine yellow and green taxi data\n",
    "combined = yellow_combined.unionByName(green_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly handling\n",
    "Filter out anomaly with business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the combined data\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count: Min = 0, Max = 9\n",
      "trip_distance: Min = 0.0, Max = 345729.44\n",
      "fare_amount: Min = -1087.3, Max = 386983.63\n",
      "extra: Min = -39.17, Max = 10002.5\n",
      "mta_tax: Min = -0.5, Max = 52.09\n",
      "tip_amount: Min = -330.88, Max = 4174.0\n",
      "tolls_amount: Min = -91.3, Max = 1702.88\n",
      "improvement_surcharge: Min = -1.0, Max = 1.0\n",
      "total_amount: Min = -1094.05, Max = 386987.63\n",
      "congestion_surcharge: Min = -2.75, Max = 2.75\n",
      "ehail_fee: Min = None, Max = None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "columns_to_check = [\n",
    "    'passenger_count', \n",
    "    'trip_distance', \n",
    "    'fare_amount', \n",
    "    'extra', \n",
    "    'mta_tax', \n",
    "    'tip_amount', \n",
    "    'tolls_amount', \n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'ehail_fee'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store min and max for each column\n",
    "min_max_dict = {col: df.agg(min(col).alias(f\"min_{col}\"), max(col).alias(f\"max_{col}\")).collect()[0] for col in columns_to_check}\n",
    "\n",
    "# Print the results\n",
    "for col, values in min_max_dict.items():\n",
    "    print(f\"{col}: Min = {values[f'min_{col}']}, Max = {values[f'max_{col}']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply all filters in a single chain\n",
    "combined = combined.filter(\n",
    "    # Filter out rows with passenger count greater than 6 or less than 1\n",
    "    (col(\"passenger_count\").between(1, 6)) &\n",
    "    # Filter out rows with fare amount less than 3\n",
    "    (col(\"fare_amount\") >= 3) &\n",
    "    # Filter out rows with trip distance less than 0.5\n",
    "    (col(\"trip_distance\") >= 0.5) &\n",
    "    # Filter out rows with tip amount less than 0 \n",
    "    (col(\"tip_amount\") >= 0) &\n",
    "    # Filter out rows with tolls amount less than 0\n",
    "    (col(\"tolls_amount\") >= 0) &\n",
    "    # Filter out rows with extra amount less than 0\n",
    "    (col(\"extra\") >= 0) &\n",
    "    # Filter out mtax_tax less than 0\n",
    "    (col(\"mta_tax\") >= 0) &\n",
    "    # Filter out rows with improvement surcharge less than 0\n",
    "    (col(\"improvement_surcharge\") >= 0) &\n",
    "    # Filter out rows with total amount less than 3\n",
    "    (col(\"total_amount\") >= 3) &\n",
    "    # Filter out rows with congestion surcharge less than 0\n",
    "    (col(\"congestion_surcharge\") >= 0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1265:===================================================>(107 + 1) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+--------------------+------------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "|summary|           VendorID|   passenger_count|    trip_distance|       RatecodeID|store_and_fwd_flag|      PULocationID|     DOLocationID|       payment_type|      fare_amount|             extra|             mta_tax|        tip_amount|      tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|ehail_fee|\n",
      "+-------+-------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+--------------------+------------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "|  count|           34348007|          34348007|         34348007|         34348007|          34348007|          34348007|         34348007|           34348007|         34348007|          34348007|            34348007|          34348007|          34348007|             34348007|          34348007|            34348007| 33727899|\n",
      "|   mean| 1.7643737233429584|1.3775216128260368|3.736275262200619|1.878972483032276|              NULL|163.70817209277965|  163.62470672607| 1.1877425668394677|20.16844040237938|1.5942724697243706|  0.4984225760172926|3.6844448078167207|0.6280710275271816|    0.998740564481661|29.463828370024444|  2.2940902510005894|      0.0|\n",
      "| stddev|0.42438960831924216|0.8642885579879114|90.38788014702962|8.903054571663336|              NULL| 63.93949781980543|69.90636626622691|0.44800693002737174|79.42921119196342|1.8391950557374797|0.061680914963770296| 4.077292252212817| 2.270180632045215| 0.034712330221885766| 80.66274019780194|  0.6899874411894488|      0.0|\n",
      "|    min|                  1|                 1|              0.5|                1|                 N|                 1|                1|                  1|              3.0|               0.0|                 0.0|               0.0|               0.0|                  0.0|               3.0|                 0.0|      0.0|\n",
      "|    max|                  2|                 6|         161726.1|               99|                 Y|               265|              265|                  5|        386983.63|             67.33|               52.09|            999.99|           1702.88|                  1.0|         386987.63|                2.75|      0.0|\n",
      "+-------+-------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+--------------------+------------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the pick up datetime to between 2023-06 to 2024-05\n",
    "combined = combined.filter(\n",
    "    (F.col(\"pickup_datetime\") >= \"2023-06-01 00:00:00\") & \n",
    "    (F.col(\"pickup_datetime\") < \"2024-06-01 00:00:00\")\n",
    ")\n",
    "# Filter the drop off datetime to between 2023-06 to 2024-05\n",
    "combined = combined.filter(\n",
    "    (F.col(\"dropoff_datetime\") >= \"2023-06-01 00:00:00\") & \n",
    "    (F.col(\"dropoff_datetime\") < \"2024-06-01 00:00:00\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:=============================================>        (90 + 8) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|min(pickup_datetime)|max(pickup_datetime)|\n",
      "+--------------------+--------------------+\n",
      "| 2023-06-01 00:00:00| 2024-05-31 23:58:59|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check minimum and maximum datetime\n",
    "combined.select(F.min(\"pickup_datetime\"), F.max(\"pickup_datetime\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to kilometers for the trip distance\n",
    "combined = combined.withColumn(\"trip_distance_km\", F.col(\"trip_distance\") * 1.60934)\n",
    "# drop the trip_distance column\n",
    "combined = combined.drop(\"trip_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|\n",
      "+-------------------+-------------------+\n",
      "|2023-06-01 00:08:48|2023-06-01 00:29:41|\n",
      "|2023-06-01 00:48:24|2023-06-01 01:07:07|\n",
      "|2023-06-01 00:54:03|2023-06-01 01:17:29|\n",
      "|2023-06-01 00:18:44|2023-06-01 00:27:18|\n",
      "|2023-06-01 00:32:36|2023-06-01 00:45:52|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined = combined.select(\"pickup_datetime\", \"dropoff_datetime\")\n",
    "combined.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|         pickup_str|        dropoff_str|\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "|2023-06-01 00:08:48|2023-06-01 00:29:41|2023-06-01 00:08:48|2023-06-01 00:29:41|\n",
      "|2023-06-01 00:48:24|2023-06-01 01:07:07|2023-06-01 00:48:24|2023-06-01 01:07:07|\n",
      "|2023-06-01 00:54:03|2023-06-01 01:17:29|2023-06-01 00:54:03|2023-06-01 01:17:29|\n",
      "|2023-06-01 00:18:44|2023-06-01 00:27:18|2023-06-01 00:18:44|2023-06-01 00:27:18|\n",
      "|2023-06-01 00:32:36|2023-06-01 00:45:52|2023-06-01 00:32:36|2023-06-01 00:45:52|\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined = combined.withColumn(\"pickup_str\", F.col(\"pickup_datetime\").cast(\"string\"))\n",
    "combined = combined.withColumn(\"dropoff_str\", F.col(\"dropoff_datetime\").cast(\"string\"))\n",
    "combined.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-----------+------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|  trip_distance_km|         pickup_str|        dropoff_str|          pickup_ts|         dropoff_ts|pickup_long|dropoff_long|\n",
      "+--------+-------------------+-------------------+---------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-----------+------------+\n",
      "|       1|2023-06-01 00:08:48|2023-06-01 00:29:41|              1|         1|                 N|         140|         238|           1|       21.9|  3.5|    0.5|       6.7|         0.0|                  1.0|        33.6|                 2.5|      0.0|          5.471756|2023-06-01 00:08:48|2023-06-01 00:29:41|2023-06-01 00:08:48|2023-06-01 00:29:41| 1685578128|  1685579381|\n",
      "|       1|2023-06-01 00:48:24|2023-06-01 01:07:07|              1|         1|                 N|         138|          97|           1|       40.8| 7.75|    0.5|      10.0|         0.0|                  1.0|       60.05|                 0.0|      0.0|16.415267999999998|2023-06-01 00:48:24|2023-06-01 01:07:07|2023-06-01 00:48:24|2023-06-01 01:07:07| 1685580504|  1685581627|\n",
      "|       2|2023-06-01 00:54:03|2023-06-01 01:17:29|              3|         1|                 N|         100|         244|           1|       39.4|  1.0|    0.5|      8.88|         0.0|                  1.0|       53.28|                 2.5|      0.0|        15.8198122|2023-06-01 00:54:03|2023-06-01 01:17:29|2023-06-01 00:54:03|2023-06-01 01:17:29| 1685580843|  1685582249|\n",
      "|       2|2023-06-01 00:18:44|2023-06-01 00:27:18|              1|         1|                 N|         137|         234|           1|        9.3|  1.0|    0.5|      0.72|         0.0|                  1.0|       15.02|                 2.5|      0.0|1.8829277999999998|2023-06-01 00:18:44|2023-06-01 00:27:18|2023-06-01 00:18:44|2023-06-01 00:27:18| 1685578724|  1685579238|\n",
      "|       1|2023-06-01 00:32:36|2023-06-01 00:45:52|              2|         1|                 N|         249|          33|           1|       18.4|  3.5|    0.5|      4.65|         0.0|                  1.0|       28.05|                 2.5|      0.0|          5.793624|2023-06-01 00:32:36|2023-06-01 00:45:52|2023-06-01 00:32:36|2023-06-01 00:45:52| 1685579556|  1685580352|\n",
      "+--------+-------------------+-------------------+---------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.withColumn(\"pickup_ts\", F.to_timestamp(\"pickup_str\"))\n",
    "test_df = test_df.withColumn(\"dropoff_ts\", F.to_timestamp(\"dropoff_str\"))\n",
    "test_df = test_df.withColumn(\"pickup_long\", F.col(\"pickup_ts\").cast(\"long\"))\n",
    "test_df = test_df.withColumn(\"dropoff_long\", F.col(\"dropoff_ts\").cast(\"long\"))\n",
    "test_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new column ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Datatype Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
