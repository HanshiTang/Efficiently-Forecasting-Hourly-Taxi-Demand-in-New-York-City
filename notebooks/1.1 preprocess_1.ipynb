{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing TLC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans the following datasets: \n",
    "1. Yellow taxi data from 2023-06 to 2024-05\n",
    "2. Green taxi data from 2023-06 to 2024-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/24 15:06:38 WARN Utils: Your hostname, Hanshis-Laptop.local resolves to a loopback address: 127.0.0.1; using 100.94.176.147 instead (on interface en0)\n",
      "24/08/24 15:06:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/24 15:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read 2023-2024 TLC data\n",
    "df = spark.read.parquet('../data/landing/tlc_data/*.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-6 to 2024-5 yellow data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "ydf_2023_6 = spark.read.parquet(path + \"/Y-2023-06.parquet\")\n",
    "ydf_2023_7 = spark.read.parquet(path + \"/Y-2023-07.parquet\")\n",
    "ydf_2023_8 = spark.read.parquet(path + \"/Y-2023-08.parquet\")\n",
    "ydf_2023_9 = spark.read.parquet(path + \"/Y-2023-09.parquet\")\n",
    "ydf_2023_10 = spark.read.parquet(path + \"/Y-2023-10.parquet\")\n",
    "ydf_2023_11 = spark.read.parquet(path + \"/Y-2023-11.parquet\")\n",
    "ydf_2023_12 = spark.read.parquet(path + \"/Y-2023-12.parquet\")\n",
    "ydf_2024_1 = spark.read.parquet(path + \"/Y-2024-01.parquet\")\n",
    "ydf_2024_2 = spark.read.parquet(path + \"/Y-2024-02.parquet\")\n",
    "ydf_2024_3 = spark.read.parquet(path + \"/Y-2024-03.parquet\")\n",
    "ydf_2024_4 = spark.read.parquet(path + \"/Y-2024-04.parquet\")\n",
    "ydf_2024_5 = spark.read.parquet(path + \"/Y-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-6 to 2024-5 green data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "gdf_2023_6 = spark.read.parquet(path + \"/G-2023-06.parquet\")\n",
    "gdf_2023_7 = spark.read.parquet(path + \"/G-2023-07.parquet\")\n",
    "gdf_2023_8 = spark.read.parquet(path + \"/G-2023-08.parquet\")\n",
    "gdf_2023_9 = spark.read.parquet(path + \"/G-2023-09.parquet\")\n",
    "gdf_2023_10 = spark.read.parquet(path + \"/G-2023-10.parquet\")\n",
    "gdf_2023_11 = spark.read.parquet(path + \"/G-2023-11.parquet\")\n",
    "gdf_2023_12 = spark.read.parquet(path + \"/G-2023-12.parquet\")\n",
    "gdf_2024_1 = spark.read.parquet(path + \"/G-2024-01.parquet\")\n",
    "gdf_2024_2 = spark.read.parquet(path + \"/G-2024-02.parquet\")\n",
    "gdf_2024_3 = spark.read.parquet(path + \"/G-2024-03.parquet\")\n",
    "gdf_2024_4 = spark.read.parquet(path + \"/G-2024-04.parquet\")\n",
    "gdf_2024_5 = spark.read.parquet(path + \"/G-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2023-06-01 00:08:48|  2023-06-01 00:29:41|              1|          3.4|         1|                 N|         140|         238|           1|       21.9|  3.5|    0.5|       6.7|         0.0|                  1.0|        33.6|                 2.5|        0.0|\n",
      "|       1| 2023-06-01 00:15:04|  2023-06-01 00:25:18|              0|          3.4|         1|                 N|          50|         151|           1|       15.6|  3.5|    0.5|       3.0|         0.0|                  1.0|        23.6|                 2.5|        0.0|\n",
      "|       1| 2023-06-01 00:48:24|  2023-06-01 01:07:07|              1|         10.2|         1|                 N|         138|          97|           1|       40.8| 7.75|    0.5|      10.0|         0.0|                  1.0|       60.05|                 0.0|       1.75|\n",
      "|       2| 2023-06-01 00:54:03|  2023-06-01 01:17:29|              3|         9.83|         1|                 N|         100|         244|           1|       39.4|  1.0|    0.5|      8.88|         0.0|                  1.0|       53.28|                 2.5|        0.0|\n",
      "|       2| 2023-06-01 00:18:44|  2023-06-01 00:27:18|              1|         1.17|         1|                 N|         137|         234|           1|        9.3|  1.0|    0.5|      0.72|         0.0|                  1.0|       15.02|                 2.5|        0.0|\n",
      "|       1| 2023-06-01 00:32:36|  2023-06-01 00:45:52|              2|          3.6|         1|                 N|         249|          33|           1|       18.4|  3.5|    0.5|      4.65|         0.0|                  1.0|       28.05|                 2.5|        0.0|\n",
      "|       2| 2023-06-01 00:31:55|  2023-06-01 00:50:51|              1|         3.08|         1|                 N|         141|         226|           1|       19.8|  1.0|    0.5|       2.0|         0.0|                  1.0|        26.8|                 2.5|        0.0|\n",
      "|       1| 2023-06-01 00:55:30|  2023-06-01 01:04:17|              2|          1.1|         1|                 N|         246|          50|           1|       10.0|  3.5|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|\n",
      "|       2| 2023-06-01 00:11:50|  2023-06-01 00:15:47|              1|         0.99|         1|                 N|         186|         234|           1|        6.5|  1.0|    0.5|      1.72|         0.0|                  1.0|       13.22|                 2.5|        0.0|\n",
      "|       2| 2023-06-01 00:25:03|  2023-06-01 00:27:54|              1|         0.73|         1|                 N|         142|         161|           2|        5.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        10.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 2023-6 yellow data \n",
    "ydf_2023_6.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2023-06-01 00:32:25|  2023-06-01 00:39:06|                 N|         1|          74|          42|              1|         0.84|        7.9|  1.0|    0.5|      2.08|         0.0|     NULL|                  1.0|       12.48|           1|        1|                 0.0|\n",
      "|       2| 2023-06-01 00:39:07|  2023-06-01 00:51:59|                 N|         1|          41|         229|              1|         4.05|       19.1|  1.0|    0.5|      3.65|         0.0|     NULL|                  1.0|        28.0|           1|        1|                2.75|\n",
      "|       2| 2023-06-01 00:35:59|  2023-06-01 00:57:06|                 N|         1|          97|          89|              1|         4.26|       23.3|  1.0|    0.5|      5.16|         0.0|     NULL|                  1.0|       30.96|           1|        1|                 0.0|\n",
      "|       2| 2023-06-01 00:50:29|  2023-06-01 01:00:32|                 N|         1|          75|         263|              1|         1.79|       12.1|  1.0|    0.5|      3.47|         0.0|     NULL|                  1.0|       20.82|           1|        1|                2.75|\n",
      "|       2| 2023-06-01 00:15:15|  2023-06-01 00:52:40|                 N|         1|          33|          48|              1|          6.4|       37.3|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|       42.55|           1|        1|                2.75|\n",
      "|       2| 2023-06-01 00:45:22|  2023-06-01 00:48:16|                 N|         1|          42|         159|              1|          0.0|        4.4|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|         6.9|           2|        1|                 0.0|\n",
      "|       2| 2023-05-31 23:00:45|  2023-05-31 23:15:21|                 N|         1|          75|         142|              1|          2.8|       17.7|  1.0|    0.5|       3.0|         0.0|     NULL|                  1.0|       25.95|           1|        1|                2.75|\n",
      "|       2| 2023-06-01 00:02:44|  2023-06-01 00:10:10|                 N|         1|          74|          75|              1|         1.67|       10.7|  1.0|    0.5|       3.3|         0.0|     NULL|                  1.0|        16.5|           1|        1|                 0.0|\n",
      "|       2| 2023-06-01 00:26:20|  2023-06-01 00:31:27|                 N|         1|          41|         152|              1|          0.9|        7.2|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|         9.7|           2|        1|                 0.0|\n",
      "|       2| 2023-06-01 00:15:00|  2023-06-01 00:18:19|                 N|         1|          95|          95|              1|         0.74|        6.5|  1.0|    0.5|       1.8|         0.0|     NULL|                  1.0|        10.8|           1|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 2023-6 green data\n",
    "gdf_2023_6.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of TLC datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38916740"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total row count for yellow taxi data from 2023-6 to 2024-5\n",
    "yellow_count = (\n",
    "    ydf_2023_6.count() + \n",
    "    ydf_2023_7.count() + \n",
    "    ydf_2023_8.count() + \n",
    "    ydf_2023_9.count() + \n",
    "    ydf_2023_10.count() + \n",
    "    ydf_2023_11.count() + \n",
    "    ydf_2023_12.count() + \n",
    "    ydf_2024_1.count() + \n",
    "    ydf_2024_2.count() + \n",
    "    ydf_2024_3.count() + \n",
    "    ydf_2024_4.count() + \n",
    "    ydf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Display the total count\n",
    "yellow_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=33751Kb max_used=33751Kb free=97320Kb\n",
      " bounds [0x000000010a1e8000, 0x000000010c318000, 0x00000001121e8000]\n",
      " total_blobs=12830 nmethods=11838 adapters=903\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "732489"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total row count for green taxi data from 2023-6 to 2024-5\n",
    "green_count = (\n",
    "    gdf_2023_6.count() + \n",
    "    gdf_2023_7.count() + \n",
    "    gdf_2023_8.count() + \n",
    "    gdf_2023_9.count() + \n",
    "    gdf_2023_10.count() + \n",
    "    gdf_2023_11.count() + \n",
    "    gdf_2023_12.count() + \n",
    "    gdf_2024_1.count() + \n",
    "    gdf_2024_2.count() + \n",
    "    gdf_2024_3.count() + \n",
    "    gdf_2024_4.count() + \n",
    "    gdf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Display the total count\n",
    "green_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39649229"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total row count for all taxi data from 2023-6 to 2024-5\n",
    "total_count = yellow_count + green_count\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema of 2024-5 yellow data\n",
    "ydf_2024_5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: long (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema of 2024-5 green data\n",
    "gdf_2024_5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns only in yellowDF: {'tpep_dropoff_datetime', 'Airport_fee', 'tpep_pickup_datetime'}\n",
      "Columns only in greenDF: {'trip_type', 'lpep_dropoff_datetime', 'ehail_fee', 'lpep_pickup_datetime'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns of each DataFrame\n",
    "columns_ydf = set(ydf_2024_5.columns)\n",
    "columns_gdf = set(gdf_2024_5.columns)\n",
    "\n",
    "# Find differences in columns\n",
    "columns_only_in_df1 = columns_ydf - columns_gdf\n",
    "columns_only_in_df2 = columns_gdf - columns_ydf\n",
    "\n",
    "print(f\"Columns only in yellowDF: {columns_only_in_df1}\")\n",
    "print(f\"Columns only in greenDF: {columns_only_in_df2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydfs = [ydf_2023_6, ydf_2023_7, ydf_2023_8, ydf_2023_9, ydf_2023_10, ydf_2023_11, ydf_2023_12, \n",
    "       ydf_2024_1, ydf_2024_2, ydf_2024_3, ydf_2024_4, ydf_2024_5]\n",
    "gdfs = [gdf_2023_6, gdf_2023_7, gdf_2023_8, gdf_2023_9, gdf_2023_10, gdf_2023_11, gdf_2023_12, \n",
    "       gdf_2024_1, gdf_2024_2, gdf_2024_3, gdf_2024_4, gdf_2024_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all yellow taxi data and green taxi data\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Combine all yellow taxi data\n",
    "yellow_combined = ydfs[0]\n",
    "for df in ydfs[1:]:\n",
    "    yellow_combined = yellow_combined.unionByName(df)\n",
    "\n",
    "# Combine all green taxi data\n",
    "green_combined = gdfs[0]\n",
    "for df in gdfs[1:]:\n",
    "    green_combined = green_combined.unionByName(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Airport_fee and trip_type columns from the combined data\n",
    "yellow_combined = yellow_combined.drop(\"Airport_fee\")\n",
    "green_combined = green_combined.drop(\"trip_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ehail fee is 0 for all yellow taxi data\n",
    "yellow_combined = yellow_combined.withColumn(\"ehail_fee\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop fare_amount less than $3 initial price \n",
    "yellow_combined = yellow_combined.filter(yellow_combined.fare_amount >= 3)\n",
    "green_combined = green_combined.filter(green_combined.fare_amount >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max passenger count should be 6 \n",
    "yellow_combined = yellow_combined.filter(yellow_combined.passenger_count <= 6)\n",
    "green_combined = green_combined.filter(green_combined.passenger_count <= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop negative trip distances\n",
    "yellow_combined = yellow_combined.filter(yellow_combined.trip_distance >= 0)\n",
    "green_combined = green_combined.filter(green_combined.trip_distance >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename datetime columns to be consistent\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "yellow_combined = yellow_combined.withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                               .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "green_combined = green_combined.withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                             .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop total_amount less than $3\n",
    "yellow_combined = yellow_combined.filter(yellow_combined.total_amount >= 3)\n",
    "green_combined = green_combined.filter(green_combined.total_amount >= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new column ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_renamed \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRatecodeID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate_codeID\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      2\u001b[0m df_renamed\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "df_renamed = df_all.withColumnRenamed(\"RatecodeID\", \"Rate_codeID\") \n",
    "df_renamed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_dropped \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_count_plus_10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_dropped\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "df_dropped = df_all.drop(\"passenger_count_plus_10\")\n",
    "df_dropped.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_all\u001b[49m\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVendorID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "df_all.groupBy(\"VendorID\").agg({\"passenger_count\": \"avg\", \"extra\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all\u001b[49m\u001b[38;5;241m.\u001b[39msample(SAMPLE_SIZE, seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20020223\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas() \n\u001b[1;32m      2\u001b[0m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "df = df_all.sample(SAMPLE_SIZE, seed = 20020223).toPandas() \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2019_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_2019_1\u001b[49m\u001b[38;5;241m.\u001b[39mprintSchema() \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m数据总量: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_2019_1\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      3\u001b[0m df_2019_1\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_2019_1' is not defined"
     ]
    }
   ],
   "source": [
    "df_2019_1.printSchema() \n",
    "print(f\"数据总量: {df_2019_1.count()}\") \n",
    "df_2019_1.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2019_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_2019_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_2019_1\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      2\u001b[0m df_2019_1 \u001b[38;5;241m=\u001b[39m df_2019_1\u001b[38;5;241m.\u001b[39mfillna({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassenger_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrip_distance\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_2019_1' is not defined"
     ]
    }
   ],
   "source": [
    "df_2019_1 = df_2019_1.dropna()\n",
    "df_2019_1 = df_2019_1.fillna({'passenger_count': 1, 'trip_distance': 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Datatype Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2019_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_2019_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_2019_1\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m df_2019_1 \u001b[38;5;241m=\u001b[39m df_2019_1\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \n\u001b[1;32m      3\u001b[0m df_2019_1 \u001b[38;5;241m=\u001b[39m df_2019_1\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_2019_1' is not defined"
     ]
    }
   ],
   "source": [
    "df_2019_1 = df_2019_1.withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"integer\"))\n",
    "df_2019_1 = df_2019_1.withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\")) \n",
    "df_2019_1 = df_2019_1.withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2019_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_2019_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_2019_1\u001b[49m\u001b[38;5;241m.\u001b[39mdropDuplicates()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_2019_1' is not defined"
     ]
    }
   ],
   "source": [
    "df_2019_1 = df_2019_1.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2019_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trip_distance_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m trip_distance_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df_2019_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_2019_1\u001b[49m\u001b[38;5;241m.\u001b[39mfilter((df_2019_1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m trip_distance_min) \u001b[38;5;241m&\u001b[39m (df_2019_1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m trip_distance_max))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_2019_1' is not defined"
     ]
    }
   ],
   "source": [
    "trip_distance_min = 0\n",
    "trip_distance_max = 100\n",
    "df_2019_1 = df_2019_1.filter((df_2019_1[\"trip_distance\"] > trip_distance_min) & (df_2019_1[\"trip_distance\"] > trip_distance_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, VectorAssembler\n\u001b[0;32m----> 2\u001b[0m assembler \u001b[38;5;241m=\u001b[39m \u001b[43mVectorAssembler\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrip_distance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrip_duration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_features \u001b[38;5;241m=\u001b[39m assembler\u001b[38;5;241m.\u001b[39mtransform(df_2019_1)\n\u001b[1;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/ml/feature.py:5358\u001b[0m, in \u001b[0;36mVectorAssembler.__init__\u001b[0;34m(self, inputCols, outputCol, handleInvalid)\u001b[0m\n\u001b[1;32m   5354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5355\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\u001b[39;00m\n\u001b[1;32m   5356\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5357\u001b[0m \u001b[38;5;28msuper\u001b[39m(VectorAssembler, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m-> 5358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.ml.feature.VectorAssembler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5360\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/ml/wrapper.py:80\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"trip_distance\", \"trip_duration\"], outputCol= \"features\")\n",
    "df_features = assembler.transform(df_2019_1)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\") \n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_scaled = scaler_model.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
