{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing TLC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans the following datasets: \n",
    "1. Yellow taxi data from 2023-06 to 2024-05\n",
    "2. Green taxi data from 2023-06 to 2024-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-2024 TLC data\n",
    "df = spark.read.parquet('../data/landing/tlc_data/*.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=28222Kb max_used=28243Kb free=102849Kb\n",
      " bounds [0x00000001069e8000, 0x00000001085a8000, 0x000000010e9e8000]\n",
      " total_blobs=11472 nmethods=10498 adapters=884\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    }
   ],
   "source": [
    "# Read 2023-6 to 2024-5 yellow data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "ydf_2023_6 = spark.read.parquet(path + \"/Y-2023-06.parquet\")\n",
    "ydf_2023_7 = spark.read.parquet(path + \"/Y-2023-07.parquet\")\n",
    "ydf_2023_8 = spark.read.parquet(path + \"/Y-2023-08.parquet\")\n",
    "ydf_2023_9 = spark.read.parquet(path + \"/Y-2023-09.parquet\")\n",
    "ydf_2023_10 = spark.read.parquet(path + \"/Y-2023-10.parquet\")\n",
    "ydf_2023_11 = spark.read.parquet(path + \"/Y-2023-11.parquet\")\n",
    "ydf_2023_12 = spark.read.parquet(path + \"/Y-2023-12.parquet\")\n",
    "ydf_2024_1 = spark.read.parquet(path + \"/Y-2024-01.parquet\")\n",
    "ydf_2024_2 = spark.read.parquet(path + \"/Y-2024-02.parquet\")\n",
    "ydf_2024_3 = spark.read.parquet(path + \"/Y-2024-03.parquet\")\n",
    "ydf_2024_4 = spark.read.parquet(path + \"/Y-2024-04.parquet\")\n",
    "ydf_2024_5 = spark.read.parquet(path + \"/Y-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-6 to 2024-5 green data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "gdf_2023_6 = spark.read.parquet(path + \"/G-2023-06.parquet\")\n",
    "gdf_2023_7 = spark.read.parquet(path + \"/G-2023-07.parquet\")\n",
    "gdf_2023_8 = spark.read.parquet(path + \"/G-2023-08.parquet\")\n",
    "gdf_2023_9 = spark.read.parquet(path + \"/G-2023-09.parquet\")\n",
    "gdf_2023_10 = spark.read.parquet(path + \"/G-2023-10.parquet\")\n",
    "gdf_2023_11 = spark.read.parquet(path + \"/G-2023-11.parquet\")\n",
    "gdf_2023_12 = spark.read.parquet(path + \"/G-2023-12.parquet\")\n",
    "gdf_2024_1 = spark.read.parquet(path + \"/G-2024-01.parquet\")\n",
    "gdf_2024_2 = spark.read.parquet(path + \"/G-2024-02.parquet\")\n",
    "gdf_2024_3 = spark.read.parquet(path + \"/G-2024-03.parquet\")\n",
    "gdf_2024_4 = spark.read.parquet(path + \"/G-2024-04.parquet\")\n",
    "gdf_2024_5 = spark.read.parquet(path + \"/G-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLC datasets inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total green count is 732489.\n",
      "The total yellow count is 38916740.\n",
      "The total count is 39649229.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total row count for yellow taxi data from 2023-6 to 2024-5\n",
    "yellow_count = (\n",
    "    ydf_2023_6.count() + \n",
    "    ydf_2023_7.count() + \n",
    "    ydf_2023_8.count() + \n",
    "    ydf_2023_9.count() + \n",
    "    ydf_2023_10.count() + \n",
    "    ydf_2023_11.count() + \n",
    "    ydf_2023_12.count() + \n",
    "    ydf_2024_1.count() + \n",
    "    ydf_2024_2.count() + \n",
    "    ydf_2024_3.count() + \n",
    "    ydf_2024_4.count() + \n",
    "    ydf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Calculate the total row count for green taxi data from 2023-6 to 2024-5\n",
    "green_count = (\n",
    "    gdf_2023_6.count() + \n",
    "    gdf_2023_7.count() + \n",
    "    gdf_2023_8.count() + \n",
    "    gdf_2023_9.count() + \n",
    "    gdf_2023_10.count() + \n",
    "    gdf_2023_11.count() + \n",
    "    gdf_2023_12.count() + \n",
    "    gdf_2024_1.count() + \n",
    "    gdf_2024_2.count() + \n",
    "    gdf_2024_3.count() + \n",
    "    gdf_2024_4.count() + \n",
    "    gdf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Display the green count\n",
    "print(f\"The total green count is {green_count}.\")\n",
    "\n",
    "# Display the yellow count\n",
    "print(f\"The total yellow count is {yellow_count}.\")\n",
    "\n",
    "# Calculate the total row count for all taxi data from 2023-6 to 2024-5\n",
    "total_count = yellow_count + green_count\n",
    "# Display the total count\n",
    "print(f\"The total count is {total_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns only in yellowDF: {'Airport_fee', 'tpep_dropoff_datetime', 'tpep_pickup_datetime'}\n",
      "Columns only in greenDF: {'lpep_pickup_datetime', 'trip_type', 'lpep_dropoff_datetime', 'ehail_fee'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns of each DataFrame\n",
    "columns_ydf = set(ydf_2024_5.columns)\n",
    "columns_gdf = set(gdf_2024_5.columns)\n",
    "\n",
    "# Find differences in columns\n",
    "columns_only_in_df1 = columns_ydf - columns_gdf\n",
    "columns_only_in_df2 = columns_gdf - columns_ydf\n",
    "\n",
    "print(f\"Columns only in yellowDF: {columns_only_in_df1}\")\n",
    "print(f\"Columns only in greenDF: {columns_only_in_df2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in yellowDF: 19\n",
      "Number of features in greenDF: 20\n"
     ]
    }
   ],
   "source": [
    "# Report number of features in each DataFrame\n",
    "print(f\"Number of features in yellowDF: {len(ydf_2024_5.columns)}\")\n",
    "print(f\"Number of features in greenDF: {len(gdf_2024_5.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets \n",
    "ydfs = [ydf_2023_6, ydf_2023_7, ydf_2023_8, ydf_2023_9, ydf_2023_10, ydf_2023_11, ydf_2023_12, \n",
    "       ydf_2024_1, ydf_2024_2, ydf_2024_3, ydf_2024_4, ydf_2024_5]\n",
    "gdfs = [gdf_2023_6, gdf_2023_7, gdf_2023_8, gdf_2023_9, gdf_2023_10, gdf_2023_11, gdf_2023_12, \n",
    "       gdf_2024_1, gdf_2024_2, gdf_2024_3, gdf_2024_4, gdf_2024_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "# Function to union two DataFrames\n",
    "def union_dfs(df1, df2):\n",
    "    return df1.unionByName(df2)\n",
    "\n",
    "# Combine all yellow taxi data\n",
    "yellow_combined = reduce(union_dfs, ydfs)\n",
    "\n",
    "# Combine all green taxi data\n",
    "green_combined = reduce(union_dfs, gdfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 03:54:44 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-----------------+----------------+---------------------+------------------+--------------------+-------------------+\n",
      "|summary|          VendorID|   passenger_count|    trip_distance|       RatecodeID|store_and_fwd_flag|      PULocationID|     DOLocationID|      payment_type|       fare_amount|             extra|            mta_tax|       tip_amount|    tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|        Airport_fee|\n",
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-----------------+----------------+---------------------+------------------+--------------------+-------------------+\n",
      "|  count|          38916740|          36470845|         38916740|         36470845|          36470845|          38916740|         38916740|          38916740|          38916740|          38916740|           38916740|         38916740|        38916740|             38916740|          38916740|            36470845|           36470845|\n",
      "|   mean|1.7501221325321699| 1.358100422405897|4.335654643734171|1.916763760203527|              NULL|164.85808767640867|163.8784072869413|1.1486949317954176|19.423297645693193| 1.483277365216099|0.48366842340853844|3.432825563241012|0.58191261061437|   0.9756503782176966|28.233441648772605|  2.2551156341455756|0.15013224810118878|\n",
      "| stddev|0.4364434190351839|0.8686641826544039|291.0124916327774| 9.02114153144218|              NULL| 64.08273002484995|69.69262070009012|0.5963151527061763| 92.67011115103949|2.4387757316601157|0.11672837920257821|4.148562268712141|2.23941114867875|  0.21568366505028796| 93.83428471852012|  0.8222473575330497| 0.5009239971086095|\n",
      "|    min|                 1|                 0|              0.0|                1|                 N|                 1|                1|                 0|           -1087.3|            -39.17|               -0.5|          -330.88|           -91.3|                 -1.0|          -1094.05|                -2.5|              -1.75|\n",
      "|    max|                 6|                 9|        345729.44|               99|                 Y|               265|              265|                 5|         386983.63|           10002.5|              52.09|           4174.0|         1702.88|                  1.0|         386987.63|                2.75|               1.75|\n",
      "+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+-----------------+----------------+---------------------+------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:=====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+---------+---------------------+------------------+------------------+-------------------+--------------------+\n",
      "|summary|          VendorID|store_and_fwd_flag|       RatecodeID|     PULocationID|     DOLocationID|   passenger_count|     trip_distance|      fare_amount|             extra|            mta_tax|        tip_amount|      tolls_amount|ehail_fee|improvement_surcharge|      total_amount|      payment_type|          trip_type|congestion_surcharge|\n",
      "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+---------+---------------------+------------------+------------------+-------------------+--------------------+\n",
      "|  count|            732489|            687452|           687452|           732489|           732489|            687452|            732489|           732489|            732489|             732489|            732489|            732489|        0|               732489|            732489|            687452|             687398|              687452|\n",
      "|   mean|1.8743080100861582|              NULL|1.184039031088716|97.20224194493024|140.0183415723649|1.3024560260207259|21.278328807668085|18.51681496923479|0.8998953567903408| 0.5678959684036211|2.4147240299855293|0.2501005202808494|     NULL|   0.9853301551286152|24.195434197646332| 1.330407068420777| 1.0393731142656801|  0.7664738774488983|\n",
      "| stddev|0.3315021321395578|              NULL|1.187830955858815|58.66257523361039|76.37922991998644| 0.947010667407842|1096.4845122711827|19.18914000738103|1.3702476448019112|0.37912600001386465| 3.329785423272552|1.3750869592947366|     NULL|   0.1389113535599435| 20.99632089533045|0.5015365398338647|0.19448117431266207|   1.232505647577715|\n",
      "|    min|                 1|                 N|                1|                1|                1|                 0|               0.0|           -500.0|              -6.0|               -0.5|             -65.0|             -6.94|     NULL|                 -1.0|            -501.0|                 1|                  1|               -2.75|\n",
      "|    max|                 2|                 Y|               99|              265|              265|                 9|         278990.28|           4003.0|              12.0|               4.25|            343.24|              51.0|     NULL|                  1.0|            4004.5|                 5|                  2|                2.75|\n",
      "+-------+------------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+------------------+------------------+---------+---------------------+------------------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show descriptive statistics for taxi data\n",
    "yellow_combined.describe().show()\n",
    "green_combined.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify the columns of the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Airport_fee and trip_type columns from the combined data\n",
    "yellow_combined = yellow_combined.drop(\"Airport_fee\")\n",
    "green_combined = green_combined.drop(\"trip_type\")\n",
    "\n",
    "# Set ehail_fee to 0 for yellow taxi data\n",
    "yellow_combined = yellow_combined.withColumn(\"ehail_fee\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tpep: Taxicab Passenger Enhancement Program for yellow taxi <br> \n",
    "lpep: Livery Passenger Enhancement Program for green taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename datetime columns to be consistent\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "yellow_combined = yellow_combined.withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                               .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "green_combined = green_combined.withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                             .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine yellow and green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine yellow and green taxi data\n",
    "combined = yellow_combined.unionByName(green_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly handling\n",
    "Filter out anomaly with business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the combined data\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 230:>                                                       (0 + 8) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count: Min = 0, Max = 9\n",
      "trip_distance: Min = 0.0, Max = 345729.44\n",
      "fare_amount: Min = -1087.3, Max = 386983.63\n",
      "extra: Min = -39.17, Max = 10002.5\n",
      "mta_tax: Min = -0.5, Max = 52.09\n",
      "tip_amount: Min = -330.88, Max = 4174.0\n",
      "tolls_amount: Min = -91.3, Max = 1702.88\n",
      "improvement_surcharge: Min = -1.0, Max = 1.0\n",
      "total_amount: Min = -1094.05, Max = 386987.63\n",
      "congestion_surcharge: Min = -2.75, Max = 2.75\n",
      "ehail_fee: Min = None, Max = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "columns_to_check = [\n",
    "    'passenger_count', \n",
    "    'trip_distance', \n",
    "    'fare_amount', \n",
    "    'extra', \n",
    "    'mta_tax', \n",
    "    'tip_amount', \n",
    "    'tolls_amount', \n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'ehail_fee'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store min and max for each column\n",
    "min_max_dict = {col: df.agg(min(col).alias(f\"min_{col}\"), max(col).alias(f\"max_{col}\")).collect()[0] for col in columns_to_check}\n",
    "\n",
    "# Print the results\n",
    "for col, values in min_max_dict.items():\n",
    "    print(f\"{col}: Min = {values[f'min_{col}']}, Max = {values[f'max_{col}']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply all filters in a single chain\n",
    "combined = combined.filter(\n",
    "    # Filter out rows with passenger count greater than 6 or less than 1\n",
    "    (col(\"passenger_count\").between(1, 6)) &\n",
    "    # Filter out rows with fare amount less than 3\n",
    "    (col(\"fare_amount\") >= 3) &\n",
    "    # Filter out rows with trip distance less than 0.5\n",
    "    (col(\"trip_distance\") >= 0.5) &\n",
    "    # Filter out rows with tip amount less than 0 \n",
    "    (col(\"tip_amount\") >= 0) &\n",
    "    # Filter out rows with tolls amount less than 0\n",
    "    (col(\"tolls_amount\") >= 0) &\n",
    "    # Filter out rows with extra amount less than 0\n",
    "    (col(\"extra\") >= 0) &\n",
    "    # Filter out mtax_tax less than 0\n",
    "    (col(\"mta_tax\") >= 0) &\n",
    "    # Filter out rows with improvement surcharge less than 0\n",
    "    (col(\"improvement_surcharge\") >= 0) &\n",
    "    # Filter out rows with total amount less than 3\n",
    "    (col(\"total_amount\") >= 3) &\n",
    "    # Filter out rows with congestion surcharge less than 0\n",
    "    (col(\"congestion_surcharge\") >= 0) &\n",
    "    # Filter the pick up datetime to between 2023-06 to 2024-05\n",
    "    (col(\"pickup_datetime\").between(\"2023-06-01 00:00:00\", \"2024-05-31 00:00:00\")) &\n",
    "    # Filter the drop off datetime to between 2023-06 to 2024-05\n",
    "    (col(\"dropoff_datetime\").between(\"2023-06-01 00:00:00\", \"2024-05-31 00:00:00\"))\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 233:====================================================>(107 + 1) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+-------------------+-----------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "|summary|           VendorID|   passenger_count|     trip_distance|        RatecodeID|store_and_fwd_flag|      PULocationID|      DOLocationID|       payment_type|      fare_amount|             extra|            mta_tax|       tip_amount|      tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|ehail_fee|\n",
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+-------------------+-----------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "|  count|           34246849|          34246849|          34246849|          34246849|          34246849|          34246849|          34246849|           34246849|         34246849|          34246849|           34246849|         34246849|          34246849|             34246849|          34246849|            34246849| 33628519|\n",
      "|   mean|  1.764397565451934|1.3775750288734592|3.7368405437823986|1.8771504204664202|              NULL|163.70584537573077|163.62511537922802| 1.1877447761690425|20.16817040277223|1.5937434404549153| 0.4984262385716128|3.684432027308626|0.6280432985236338|   0.9987787577187041|29.463097859613256|  2.2941264026947414|      0.0|\n",
      "| stddev|0.42437475495343846|0.8643942380950901| 90.52093659447955| 8.893206865096955|              NULL|63.937419809589926|  69.9060952480568|0.44795949741540453|79.54012111945029| 1.839071902083761|0.06169133034646834|4.077097720746317|2.2700497717788473| 0.034159732626038344| 80.77181193801556|  0.6899331036906534|      0.0|\n",
      "|    min|                  1|                 1|               0.5|                 1|                 N|                 1|                 1|                  1|              3.0|               0.0|                0.0|              0.0|               0.0|                  0.0|               3.0|                 0.0|      0.0|\n",
      "|    max|                  2|                 6|          161726.1|                99|                 Y|               265|               265|                  5|        386983.63|             67.33|              52.09|           999.99|           1702.88|                  1.0|         386987.63|                2.75|      0.0|\n",
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+-------------------+-----------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:============================================>         (89 + 8) / 108]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|min_pickup_datetime|max_pickup_datetime|min_dropoff_datetime|max_dropoff_datetime|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|2023-06-01 00:00:00|2024-05-30 23:56:52| 2023-06-01 00:02:46| 2024-05-31 00:00:00|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show range for pickup_datetime and dropoff_datetime\n",
    "combined.select(\n",
    "    min(\"pickup_datetime\").alias(\"min_pickup_datetime\"), \n",
    "    max(\"pickup_datetime\").alias(\"max_pickup_datetime\"),\n",
    "    min(\"dropoff_datetime\").alias(\"min_dropoff_datetime\"), \n",
    "    max(\"dropoff_datetime\").alias(\"max_dropoff_datetime\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "|       1|2023-06-01 00:08:48|2023-06-01 00:29:41|              1|          3.4|         1|                 N|         140|         238|           1|       21.9|  3.5|    0.5|       6.7|         0.0|                  1.0|        33.6|                 2.5|      0.0|         21.0|\n",
      "|       1|2023-06-01 00:48:24|2023-06-01 01:07:07|              1|         10.2|         1|                 N|         138|          97|           1|       40.8| 7.75|    0.5|      10.0|         0.0|                  1.0|       60.05|                 0.0|      0.0|         19.0|\n",
      "|       2|2023-06-01 00:54:03|2023-06-01 01:17:29|              3|         9.83|         1|                 N|         100|         244|           1|       39.4|  1.0|    0.5|      8.88|         0.0|                  1.0|       53.28|                 2.5|      0.0|         23.0|\n",
      "|       2|2023-06-01 00:18:44|2023-06-01 00:27:18|              1|         1.17|         1|                 N|         137|         234|           1|        9.3|  1.0|    0.5|      0.72|         0.0|                  1.0|       15.02|                 2.5|      0.0|          9.0|\n",
      "|       1|2023-06-01 00:32:36|2023-06-01 00:45:52|              2|          3.6|         1|                 N|         249|          33|           1|       18.4|  3.5|    0.5|      4.65|         0.0|                  1.0|       28.05|                 2.5|      0.0|         13.0|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert pickup and dropoff timestamps to string format\n",
    "combined = combined.withColumn(\"pickup_str\", F.col(\"pickup_datetime\").cast(\"string\"))\n",
    "combined = combined.withColumn(\"dropoff_str\", F.col(\"dropoff_datetime\").cast(\"string\"))\n",
    "\n",
    "# Step 2: Convert the strings back to timestamps\n",
    "combined = combined.withColumn(\"pickup_ts\", F.to_timestamp(\"pickup_str\"))\n",
    "combined = combined.withColumn(\"dropoff_ts\", F.to_timestamp(\"dropoff_str\"))\n",
    "\n",
    "# Step 3: Convert the timestamps to long (Unix epoch seconds)\n",
    "combined = combined.withColumn(\"pickup_long\", F.col(\"pickup_ts\").cast(\"long\"))\n",
    "combined = combined.withColumn(\"dropoff_long\", F.col(\"dropoff_ts\").cast(\"long\"))\n",
    "\n",
    "# Step 4: Calculate the trip duration in minutes\n",
    "combined = combined.withColumn(\"trip_duration\", F.round((F.col(\"dropoff_long\") - F.col(\"pickup_long\")) / 60))\n",
    "\n",
    "# Drop intermediate columns if no longer needed\n",
    "combined = combined.drop(\"pickup_str\", \"dropoff_str\", \"pickup_ts\", \"dropoff_ts\", \"pickup_long\", \"dropoff_long\")\n",
    "\n",
    "# Show the result\n",
    "combined.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col as pyspark_col\n",
    "\n",
    "# Filter out rows with trip duration less than 1 minute or greater than 180 minutes\n",
    "combined = combined.filter(\n",
    "    (pyspark_col(\"trip_duration\") >= 1) & (pyspark_col(\"trip_duration\") <= 180)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 04:05:03 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:05:03 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:05:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:05:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:05:33 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:05:33 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:05:56 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:05:56 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:05:59 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:05:59 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:03 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:03 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:10 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:10 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:13 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:13 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:16 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:16 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:16 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:16 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:20 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:20 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:20 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:20 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:23 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:23 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:24 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:24 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:27 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:27 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:27 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:27 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:30 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:33 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:06:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:07:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:07:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:07:30 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:07:30 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/08/25 04:07:36 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/08/25 04:07:36 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "ERROR:root:KeyboardInterrupt while sending command.8][Stage 243:>(0 + 0) / 108]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# export the cleaned data to raw folder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/raw/tlc_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# export the cleaned data to raw folder\n",
    "combined.write.parquet(\"../data/raw/tlc_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Datatype Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
