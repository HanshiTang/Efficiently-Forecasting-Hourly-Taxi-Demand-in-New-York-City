{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing_final_1\n",
    "This notebook merges 3 preprocessed datasets and prepares for EDA_daily and geospatial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/30 11:41:35 WARN Utils: Your hostname, Hanshis-Laptop.local resolves to a loopback address: 127.0.0.1; using 172.16.119.21 instead (on interface en0)\n",
      "24/08/30 11:41:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/30 11:41:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/30 11:41:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from preprocessed tlc data\n",
    "tdf = spark.read.parquet(\"../data/raw/tlc_df.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed weather data\n",
    "wdf = spark.read.csv(\"../data/raw/NYC_weather_raw.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed event data\n",
    "edf = spark.read.parquet(\"../data/raw/NYC_Permitted_Event_Information_Historical.parquet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the zones DataFrame\n",
    "zones = spark.read.csv(\"../data/landing/external/taxi_zones.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tdf DataFrame with zones to get the borough for PULocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'PUBorough'),\n",
    "               tdf['PULocationID'] == zones['LocationID'], 'left').drop('LocationID')\n",
    "\n",
    "# Join the tdf DataFrame with zones to get the borough for DOLocationID\n",
    "tdf = tdf.join(zones.select('LocationID', 'borough').withColumnRenamed('borough', 'DOBorough'),\n",
    "               tdf['DOLocationID'] == zones['LocationID'], 'left').drop('LocationID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup_date and pickup_hour to a timestamp and create Time column\n",
    "tdf = tdf.withColumn(\n",
    "    'Time', \n",
    "    to_timestamp(concat_ws(' ', col('pickup_date')))\n",
    ")\n",
    "\n",
    "df_daily_agg = tdf.groupBy(['pickup_date', \"PUBorough\"]).agg({\n",
    "    '*': 'count',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the number of events per date and borough\n",
    "edf_daily = edf.groupBy('Start Date', 'Event Borough').agg(count('Event ID').alias('Number of Events'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create unified datetime columns in each dataset\n",
    "\n",
    "# For tdf (taxi dataset)\n",
    "df_daily_agg = df_daily_agg.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"pickup_date\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "# For edf (event dataset)\n",
    "edf_daily = edf_daily.withColumn(\"datetime\", to_timestamp(concat_ws(\" \", col(\"Start Date\")), \"yyyy-MM-dd\"))\n",
    "edf_daily = edf_daily.withColumnRenamed(\"Event Borough\", \"PUBorough\")\n",
    "\n",
    "# For wdf (weather dataset)\n",
    "wdf = wdf.withColumn(\"datetime\", to_timestamp(col(\"DATE\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Step 2: Join tdf (taxi dataset) with edf (event dataset) on datetime and borough\n",
    "tdf_edf = df_daily_agg.join(edf_daily, on=[\"datetime\", \"PUBorough\"], how=\"left\")\n",
    "\n",
    "# Step 3: Join the result with the weather dataset on datetime only (since weather data is from one station)\n",
    "final_df = tdf_edf.join(wdf, \n",
    "                        on=\"datetime\", \n",
    "                        how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'pickup_date' and 'PUBorough' to get the sum of trips and average weather data\n",
    "daily_demand = clean_df.groupby(['pickup_date', 'PUBorough']).agg({\n",
    "    'daily_trip_count': 'sum',\n",
    "    'TMP': 'mean',\n",
    "    'VIS': 'mean',\n",
    "    'Number of Events': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns to match the desired output\n",
    "daily_demand.rename(columns={\n",
    "    'daily_trip_count': 'total_daily_trips',\n",
    "    'TMP': 'avg_temperature',\n",
    "    'VIS': 'avg_visibility',\n",
    "    'Number of Events': 'total_events'\n",
    "}, inplace=True)\n",
    "\n",
    "# Round the average temperature and visibility to 1 decimal place\n",
    "daily_demand['avg_temperature'] = daily_demand['avg_temperature'].round(1)\n",
    "daily_demand['avg_visibility'] = daily_demand['avg_visibility'].round(1)\n",
    "\n",
    "# Show the first 5 rows of the aggregated result\n",
    "print(daily_demand.head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
