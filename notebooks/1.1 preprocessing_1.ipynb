{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing TLC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans the following datasets: \n",
    "1. Yellow taxi data from 2023-12 to 2024-05\n",
    "2. Green taxi data from 2023-12 to 2024-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 19:18:55 WARN Utils: Your hostname, Hanshis-Laptop.local resolves to a loopback address: 127.0.0.1; using 10.12.200.32 instead (on interface en0)\n",
      "24/08/26 19:18:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/26 19:18:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-2024 TLC data\n",
    "df = spark.read.parquet('../data/landing/tlc_data/*.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-6 to 2024-5 yellow data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "ydf_2023_12 = spark.read.parquet(path + \"/Y-2023-12.parquet\")\n",
    "ydf_2024_1 = spark.read.parquet(path + \"/Y-2024-01.parquet\")\n",
    "ydf_2024_2 = spark.read.parquet(path + \"/Y-2024-02.parquet\")\n",
    "ydf_2024_3 = spark.read.parquet(path + \"/Y-2024-03.parquet\")\n",
    "ydf_2024_4 = spark.read.parquet(path + \"/Y-2024-04.parquet\")\n",
    "ydf_2024_5 = spark.read.parquet(path + \"/Y-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-12 to 2024-5 green data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "gdf_2023_12 = spark.read.parquet(path + \"/G-2023-12.parquet\")\n",
    "gdf_2024_1 = spark.read.parquet(path + \"/G-2024-01.parquet\")\n",
    "gdf_2024_2 = spark.read.parquet(path + \"/G-2024-02.parquet\")\n",
    "gdf_2024_3 = spark.read.parquet(path + \"/G-2024-03.parquet\")\n",
    "gdf_2024_4 = spark.read.parquet(path + \"/G-2024-04.parquet\")\n",
    "gdf_2024_5 = spark.read.parquet(path + \"/G-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLC datasets inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total green count is 349274.\n",
      "The total yellow count is 20169467.\n",
      "The total count is 20518741.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total row count for yellow taxi data from 2023-6 to 2024-5\n",
    "yellow_count = (\n",
    "    ydf_2023_12.count() + \n",
    "    ydf_2024_1.count() + \n",
    "    ydf_2024_2.count() + \n",
    "    ydf_2024_3.count() + \n",
    "    ydf_2024_4.count() + \n",
    "    ydf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Calculate the total row count for green taxi data from 2023-6 to 2024-5\n",
    "green_count = (\n",
    "    gdf_2023_12.count() + \n",
    "    gdf_2024_1.count() + \n",
    "    gdf_2024_2.count() + \n",
    "    gdf_2024_3.count() + \n",
    "    gdf_2024_4.count() + \n",
    "    gdf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Display the green count\n",
    "print(f\"The total green count is {green_count}.\")\n",
    "\n",
    "# Display the yellow count\n",
    "print(f\"The total yellow count is {yellow_count}.\")\n",
    "\n",
    "# Calculate the total row count for all taxi data from 2023-6 to 2024-5\n",
    "total_count = yellow_count + green_count\n",
    "# Display the total count\n",
    "print(f\"The total count is {total_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns only in yellowDF: {'tpep_dropoff_datetime', 'tpep_pickup_datetime', 'Airport_fee'}\n",
      "Columns only in greenDF: {'ehail_fee', 'trip_type', 'lpep_pickup_datetime', 'lpep_dropoff_datetime'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns of each DataFrame\n",
    "columns_ydf = set(ydf_2024_5.columns)\n",
    "columns_gdf = set(gdf_2024_5.columns)\n",
    "\n",
    "# Find differences in columns\n",
    "columns_only_in_df1 = columns_ydf - columns_gdf\n",
    "columns_only_in_df2 = columns_gdf - columns_ydf\n",
    "\n",
    "print(f\"Columns only in yellowDF: {columns_only_in_df1}\")\n",
    "print(f\"Columns only in greenDF: {columns_only_in_df2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in yellowDF: 19\n",
      "Number of features in greenDF: 20\n"
     ]
    }
   ],
   "source": [
    "# Report number of features in each DataFrame\n",
    "print(f\"Number of features in yellowDF: {len(ydf_2024_5.columns)}\")\n",
    "print(f\"Number of features in greenDF: {len(gdf_2024_5.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets \n",
    "ydfs = [ydf_2023_12, \n",
    "       ydf_2024_1, ydf_2024_2, ydf_2024_3, ydf_2024_4, ydf_2024_5]\n",
    "gdfs = [gdf_2023_12, \n",
    "       gdf_2024_1, gdf_2024_2, gdf_2024_3, gdf_2024_4, gdf_2024_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "# Function to union two DataFrames\n",
    "def union_dfs(df1, df2):\n",
    "    return df1.unionByName(df2)\n",
    "\n",
    "# Combine all yellow taxi data\n",
    "yellow_combined = reduce(union_dfs, ydfs)\n",
    "\n",
    "# Combine all green taxi data\n",
    "green_combined = reduce(union_dfs, gdfs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify the columns of the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Airport_fee and trip_type columns from the combined data\n",
    "yellow_combined = yellow_combined.drop(\"Airport_fee\")\n",
    "green_combined = green_combined.drop(\"trip_type\")\n",
    "\n",
    "# Set ehail_fee to 0 for yellow taxi data\n",
    "yellow_combined = yellow_combined.withColumn(\"ehail_fee\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tpep: Taxicab Passenger Enhancement Program for yellow taxi <br> \n",
    "lpep: Livery Passenger Enhancement Program for green taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename datetime columns to be consistent\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "yellow_combined = yellow_combined.withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                               .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "green_combined = green_combined.withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                             .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine yellow and green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine yellow and green taxi data\n",
    "combined = yellow_combined.unionByName(green_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly handling 1\n",
    "Filter out anomaly with business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the combined data\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=32994Kb max_used=32994Kb free=98077Kb\n",
      " bounds [0x000000010b1e8000, 0x000000010d258000, 0x00000001131e8000]\n",
      " total_blobs=12681 nmethods=11698 adapters=896\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "passenger_count: Min = 0, Max = 9\n",
      "trip_distance: Min = 0.0, Max = 345729.44\n",
      "fare_amount: Min = -1087.3, Max = 386983.63\n",
      "extra: Min = -39.17, Max = 10002.5\n",
      "mta_tax: Min = -0.5, Max = 52.09\n",
      "tip_amount: Min = -330.88, Max = 4174.0\n",
      "tolls_amount: Min = -91.3, Max = 1702.88\n",
      "improvement_surcharge: Min = -1.0, Max = 1.0\n",
      "total_amount: Min = -1094.05, Max = 386987.63\n",
      "congestion_surcharge: Min = -2.75, Max = 2.75\n",
      "ehail_fee: Min = None, Max = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "columns_to_check = [\n",
    "    'passenger_count', \n",
    "    'trip_distance', \n",
    "    'fare_amount', \n",
    "    'extra', \n",
    "    'mta_tax', \n",
    "    'tip_amount', \n",
    "    'tolls_amount', \n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'ehail_fee'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store min and max for each column\n",
    "min_max_dict = {col: df.agg(min(col).alias(f\"min_{col}\"), max(col).alias(f\"max_{col}\")).collect()[0] for col in columns_to_check}\n",
    "\n",
    "# Print the results\n",
    "for col, values in min_max_dict.items():\n",
    "    print(f\"{col}: Min = {values[f'min_{col}']}, Max = {values[f'max_{col}']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply all filters in a single chain\n",
    "combined = combined.filter(\n",
    "    # Filter out rows with passenger count greater than 6 or less than 1\n",
    "    (col(\"passenger_count\").between(1, 6)) &\n",
    "    # Filter out rows with fare amount less than 3\n",
    "    (col(\"fare_amount\") >= 3) &\n",
    "    # Filter out rows with trip distance less than 0.5 miles \n",
    "    (col(\"trip_distance\") >= 0.5) &\n",
    "    # Filter out rows with tip amount less than 0 \n",
    "    (col(\"tip_amount\") >= 0) &\n",
    "    # Filter out rows with tolls amount less than 0\n",
    "    (col(\"tolls_amount\") >= 0) &\n",
    "    # Filter out rows with extra amount less than 0\n",
    "    (col(\"extra\") >= 0) &\n",
    "    # Filter out mtax_tax less than 0\n",
    "    (col(\"mta_tax\") >= 0) &\n",
    "    # Filter out rows with improvement surcharge less than 0\n",
    "    (col(\"improvement_surcharge\") >= 0) &\n",
    "    # Filter out rows with total amount less than 3\n",
    "    (col(\"total_amount\") >= 3) &\n",
    "    # Filter out rows with congestion surcharge less than 0\n",
    "    (col(\"congestion_surcharge\") >= 0) &\n",
    "    # Filter the pick up datetime to between 2023-06 to 2024-05\n",
    "    (col(\"pickup_datetime\").between(\"2023-06-01 00:00:00\", \"2024-05-31 00:00:00\")) &\n",
    "    # Filter the drop off datetime to between 2023-06 to 2024-05\n",
    "    (col(\"dropoff_datetime\").between(\"2023-06-01 00:00:00\", \"2024-05-31 00:00:00\"))\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 19:19:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "|summary|           VendorID|   passenger_count|    trip_distance|        RatecodeID|store_and_fwd_flag|      PULocationID|     DOLocationID|       payment_type|       fare_amount|             extra|            mta_tax|        tip_amount|      tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|ehail_fee|\n",
      "+-------+-------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "|  count|           17206387|          17206387|         17206387|          17206387|          17206387|          17206387|         17206387|           17206387|          17206387|          17206387|           17206387|          17206387|          17206387|             17206387|          17206387|            17206387| 16908732|\n",
      "|   mean| 1.7654224562076861|1.3626587034221653|3.598899258746351|2.1237581137748442|              NULL|164.14228507123548|164.0507406929764| 1.1808497623585938|19.694312028434908|1.5763306584932673| 0.4983085437982999|3.6617070963239526|0.6102989151641137|   0.9977438901031359|28.937723876052882|  2.3007278924971293|      0.0|\n",
      "| stddev|0.42373450434890436|0.8435597597767445| 89.3606851840288|10.149176604281056|              NULL|63.902889730884056|69.68660911242951|0.44847542365078896|17.697540844397682|1.8278900012165353|0.06006268051252007| 4.014252175945369| 2.271850001138832|  0.04670798510934922|22.482209366617187|  0.6797659506387704|      0.0|\n",
      "|    min|                  1|                 1|              0.5|                 1|                 N|                 1|                1|                  1|               3.0|               0.0|                0.0|               0.0|               0.0|                  0.0|               3.0|                 0.0|      0.0|\n",
      "|    max|                  2|                 6|         161726.1|                99|                 Y|               265|              265|                  5|           2320.11|             51.68|              35.84|            999.99|           1702.88|                  1.0|           2372.79|                2.75|      0.0|\n",
      "+-------+-------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:====================================================>   (51 + 3) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|min_pickup_datetime|max_pickup_datetime|min_dropoff_datetime|max_dropoff_datetime|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|2023-11-24 21:03:34|2024-05-30 23:56:52| 2023-11-24 21:22:29| 2024-05-31 00:00:00|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show range for pickup_datetime and dropoff_datetime\n",
    "combined.select(\n",
    "    min(\"pickup_datetime\").alias(\"min_pickup_datetime\"), \n",
    "    max(\"pickup_datetime\").alias(\"max_pickup_datetime\"),\n",
    "    min(\"dropoff_datetime\").alias(\"min_dropoff_datetime\"), \n",
    "    max(\"dropoff_datetime\").alias(\"max_dropoff_datetime\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'store_and_fwd_flag' to boolean\n",
    "combined = combined.withColumn(\"store_and_fwd_flag\", when(col(\"store_and_fwd_flag\") == \"Y\", True).otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "|       1|2023-12-01 00:59:44|2023-12-01 01:13:22|              2|          2.2|         1|             false|         114|         186|           1|       13.5|  3.5|    0.5|       3.0|         0.0|                  1.0|        21.5|                 2.5|      0.0|         14.0|\n",
      "|       2|2023-12-01 00:22:17|2023-12-01 00:30:59|              1|         0.66|         1|             false|          79|          79|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.2|                 2.5|      0.0|          9.0|\n",
      "|       2|2023-12-01 00:18:16|2023-12-01 00:25:32|              2|          2.2|         1|             false|         229|         263|           1|       11.4|  1.0|    0.5|       2.0|         0.0|                  1.0|        18.4|                 2.5|      0.0|          7.0|\n",
      "|       2|2023-12-01 00:17:09|2023-12-01 00:33:31|              1|         5.33|         1|             false|          45|         162|           1|       24.7|  1.0|    0.5|       3.0|         0.0|                  1.0|        32.7|                 2.5|      0.0|         16.0|\n",
      "|       2|2023-12-01 00:40:49|2023-12-01 00:44:10|              1|         0.76|         1|             false|         170|         107|           1|        5.8|  1.0|    0.5|       1.0|         0.0|                  1.0|        11.8|                 2.5|      0.0|          3.0|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert pickup and dropoff timestamps to string format\n",
    "combined = combined.withColumn(\"pickup_str\", F.col(\"pickup_datetime\").cast(\"string\"))\n",
    "combined = combined.withColumn(\"dropoff_str\", F.col(\"dropoff_datetime\").cast(\"string\"))\n",
    "\n",
    "# Step 2: Convert the strings back to timestamps\n",
    "combined = combined.withColumn(\"pickup_ts\", F.to_timestamp(\"pickup_str\"))\n",
    "combined = combined.withColumn(\"dropoff_ts\", F.to_timestamp(\"dropoff_str\"))\n",
    "\n",
    "# Step 3: Convert the timestamps to long (Unix epoch seconds)\n",
    "combined = combined.withColumn(\"pickup_long\", F.col(\"pickup_ts\").cast(\"long\"))\n",
    "combined = combined.withColumn(\"dropoff_long\", F.col(\"dropoff_ts\").cast(\"long\"))\n",
    "\n",
    "# Step 4: Calculate the trip duration in minutes\n",
    "combined = combined.withColumn(\"trip_duration\", F.round((F.col(\"dropoff_long\") - F.col(\"pickup_long\")) / 60))\n",
    "\n",
    "# Drop intermediate columns if no longer needed\n",
    "combined = combined.drop(\"pickup_str\", \"dropoff_str\", \"pickup_ts\", \"dropoff_ts\", \"pickup_long\", \"dropoff_long\")\n",
    "\n",
    "# Show the result\n",
    "combined.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = false)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check datatypes\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Northern end of bronx to southern end of staten island is below 50 miles </br>\n",
    "2 hours should be sufficient for travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col as pyspark_col\n",
    "\n",
    "# Filter out rows with trip duration greater than 120 minutes\n",
    "combined = combined.filter(\n",
    "    (pyspark_col(\"trip_duration\") <= 120)\n",
    ")\n",
    "\n",
    "# Filter out rows with trip distance greater than 50 miles \n",
    "combined = combined.filter(\n",
    "    pyspark_col(\"trip_distance\") <= 50\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values & duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dulicate rows\n",
    "combined = combined.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "combined = combined.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly handling by statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 99% quantile of total_amount\n",
    "total_amount_quantile = combined.approxQuantile(\"total_amount\", [0.99], 0.01)[0]\n",
    "combined = combined.filter(pyspark_col(\"total_amount\") <= total_amount_quantile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:===================================>                    (10 + 6) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows remaining after filtering: 82.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check the percentage of rows that remain after filtering\n",
    "original_count = total_count\n",
    "filtered_count = combined.count()\n",
    "percentage_remaining = (filtered_count / original_count) * 100\n",
    "print(f\"Percentage of rows remaining after filtering: {percentage_remaining:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export file to raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# export the combined data to parquet\n",
    "combined.write.mode(\"overwrite\").parquet(\"../data/raw/tlc_data/combined.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
