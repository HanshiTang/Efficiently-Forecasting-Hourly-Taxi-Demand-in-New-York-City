{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing TLC data\n",
    "This notebook cleans the following datasets: \n",
    "1. Yellow taxi data from 2023-12 to 2024-05\n",
    "2. Green taxi data from 2023-12 to 2024-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-2024 TLC data\n",
    "df = spark.read.parquet('../data/landing/tlc_data/*.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-6 to 2024-5 yellow data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "ydf_2023_12 = spark.read.parquet(path + \"/Y-2023-12.parquet\")\n",
    "ydf_2024_1 = spark.read.parquet(path + \"/Y-2024-01.parquet\")\n",
    "ydf_2024_2 = spark.read.parquet(path + \"/Y-2024-02.parquet\")\n",
    "ydf_2024_3 = spark.read.parquet(path + \"/Y-2024-03.parquet\")\n",
    "ydf_2024_4 = spark.read.parquet(path + \"/Y-2024-04.parquet\")\n",
    "ydf_2024_5 = spark.read.parquet(path + \"/Y-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 2023-12 to 2024-5 green data\n",
    "path = \"../data/landing/tlc_data\"\n",
    "gdf_2023_12 = spark.read.parquet(path + \"/G-2023-12.parquet\")\n",
    "gdf_2024_1 = spark.read.parquet(path + \"/G-2024-01.parquet\")\n",
    "gdf_2024_2 = spark.read.parquet(path + \"/G-2024-02.parquet\")\n",
    "gdf_2024_3 = spark.read.parquet(path + \"/G-2024-03.parquet\")\n",
    "gdf_2024_4 = spark.read.parquet(path + \"/G-2024-04.parquet\")\n",
    "gdf_2024_5 = spark.read.parquet(path + \"/G-2024-05.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total green count is 349274.\n",
      "The total yellow count is 20169467.\n",
      "The total count is 20518741.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total row count for yellow taxi data from 2023-6 to 2024-5\n",
    "yellow_count = (\n",
    "    ydf_2023_12.count() + \n",
    "    ydf_2024_1.count() + \n",
    "    ydf_2024_2.count() + \n",
    "    ydf_2024_3.count() + \n",
    "    ydf_2024_4.count() + \n",
    "    ydf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Calculate the total row count for green taxi data from 2023-6 to 2024-5\n",
    "green_count = (\n",
    "    gdf_2023_12.count() + \n",
    "    gdf_2024_1.count() + \n",
    "    gdf_2024_2.count() + \n",
    "    gdf_2024_3.count() + \n",
    "    gdf_2024_4.count() + \n",
    "    gdf_2024_5.count()\n",
    ")\n",
    "\n",
    "# Display the green count\n",
    "print(f\"The total green count is {green_count}.\")\n",
    "\n",
    "# Display the yellow count\n",
    "print(f\"The total yellow count is {yellow_count}.\")\n",
    "\n",
    "# Calculate the total row count for all taxi data from 2023-6 to 2024-5\n",
    "total_count = yellow_count + green_count\n",
    "# Display the total count\n",
    "print(f\"The total count is {total_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns only in yellowDF: {'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'Airport_fee'}\n",
      "Columns only in greenDF: {'ehail_fee', 'lpep_dropoff_datetime', 'trip_type', 'lpep_pickup_datetime'}\n"
     ]
    }
   ],
   "source": [
    "# Get columns of each DataFrame\n",
    "columns_ydf = set(ydf_2024_5.columns)\n",
    "columns_gdf = set(gdf_2024_5.columns)\n",
    "\n",
    "# Find differences in columns\n",
    "columns_only_in_df1 = columns_ydf - columns_gdf\n",
    "columns_only_in_df2 = columns_gdf - columns_ydf\n",
    "\n",
    "print(f\"Columns only in yellowDF: {columns_only_in_df1}\")\n",
    "print(f\"Columns only in greenDF: {columns_only_in_df2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in yellowDF: 19\n",
      "Number of features in greenDF: 20\n"
     ]
    }
   ],
   "source": [
    "# Report number of features in each DataFrame\n",
    "print(f\"Number of features in yellowDF: {len(ydf_2024_5.columns)}\")\n",
    "print(f\"Number of features in greenDF: {len(gdf_2024_5.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets \n",
    "ydfs = [ydf_2023_12, \n",
    "       ydf_2024_1, ydf_2024_2, ydf_2024_3, ydf_2024_4, ydf_2024_5]\n",
    "gdfs = [gdf_2023_12, \n",
    "       gdf_2024_1, gdf_2024_2, gdf_2024_3, gdf_2024_4, gdf_2024_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "# Function to union two DataFrames\n",
    "def union_dfs(df1, df2):\n",
    "    return df1.unionByName(df2)\n",
    "\n",
    "# Combine all yellow taxi data\n",
    "yellow_combined = reduce(union_dfs, ydfs)\n",
    "\n",
    "# Combine all green taxi data\n",
    "green_combined = reduce(union_dfs, gdfs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify the columns of the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Airport_fee and trip_type columns from the combined data\n",
    "yellow_combined = yellow_combined.drop(\"Airport_fee\")\n",
    "green_combined = green_combined.drop(\"trip_type\")\n",
    "\n",
    "# Set ehail_fee to 0 for yellow taxi data\n",
    "yellow_combined = yellow_combined.withColumn(\"ehail_fee\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tpep: Taxicab Passenger Enhancement Program for yellow taxi <br> \n",
    "lpep: Livery Passenger Enhancement Program for green taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename datetime columns to be consistent\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "yellow_combined = yellow_combined.withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                               .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "green_combined = green_combined.withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "                             .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine yellow and green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine yellow and green taxi data\n",
    "combined = yellow_combined.unionByName(green_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly handling 1\n",
    "Filter out anomaly with business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the combined data\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count: Min = 0, Max = 9\n",
      "trip_distance: Min = 0.0, Max = 345729.44\n",
      "fare_amount: Min = -1087.3, Max = 386983.63\n",
      "extra: Min = -39.17, Max = 10002.5\n",
      "mta_tax: Min = -0.5, Max = 52.09\n",
      "tip_amount: Min = -330.88, Max = 4174.0\n",
      "tolls_amount: Min = -91.3, Max = 1702.88\n",
      "improvement_surcharge: Min = -1.0, Max = 1.0\n",
      "total_amount: Min = -1094.05, Max = 386987.63\n",
      "congestion_surcharge: Min = -2.75, Max = 2.75\n",
      "ehail_fee: Min = None, Max = None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "columns_to_check = [\n",
    "    'passenger_count', \n",
    "    'trip_distance', \n",
    "    'fare_amount', \n",
    "    'extra', \n",
    "    'mta_tax', \n",
    "    'tip_amount', \n",
    "    'tolls_amount', \n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'ehail_fee'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store min and max for each column\n",
    "min_max_dict = {col: df.agg(min(col).alias(f\"min_{col}\"), max(col).alias(f\"max_{col}\")).collect()[0] for col in columns_to_check}\n",
    "\n",
    "# Print the results\n",
    "for col, values in min_max_dict.items():\n",
    "    print(f\"{col}: Min = {values[f'min_{col}']}, Max = {values[f'max_{col}']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply all filters in a single chain\n",
    "combined = combined.filter(\n",
    "    # Filter out rows with passenger count greater than 6 or less than 1\n",
    "    (col(\"passenger_count\").between(1, 6)) &\n",
    "    # Filter out rows with fare amount less than 3\n",
    "    (col(\"fare_amount\") >= 3) &\n",
    "    # Filter out rows with trip distance less than 0.5 miles \n",
    "    (col(\"trip_distance\") >= 0.5) &\n",
    "    # Filter out rows with tip amount less than 0 \n",
    "    (col(\"tip_amount\") >= 0) &\n",
    "    # Filter out rows with tolls amount less than 0\n",
    "    (col(\"tolls_amount\") >= 0) &\n",
    "    # Filter out rows with extra amount less than 0\n",
    "    (col(\"extra\") >= 0) &\n",
    "    # Filter out mtax_tax less than 0\n",
    "    (col(\"mta_tax\") >= 0) &\n",
    "    # Filter out rows with improvement surcharge less than 0\n",
    "    (col(\"improvement_surcharge\") >= 0) &\n",
    "    # Filter out rows with total amount less than 3\n",
    "    (col(\"total_amount\") >= 3) &\n",
    "    # Filter out rows with congestion surcharge less than 0\n",
    "    (col(\"congestion_surcharge\") >= 0) &\n",
    "    # Filter the pick up datetime to between 2023-06 to 2024-05\n",
    "    (col(\"pickup_datetime\").between(\"2023-06-01 00:00:00\", \"2024-05-31 00:00:00\")) &\n",
    "    # Filter the drop off datetime to between 2023-06 to 2024-05\n",
    "    (col(\"dropoff_datetime\").between(\"2023-06-01 00:00:00\", \"2024-05-31 00:00:00\"))\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'store_and_fwd_flag' to boolean\n",
    "combined = combined.withColumn(\"store_and_fwd_flag\", when(col(\"store_and_fwd_flag\") == \"Y\", True).otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|ehail_fee|trip_duration|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "|       1|2023-12-01 00:59:44|2023-12-01 01:13:22|              2|          2.2|         1|             false|         114|         186|           1|       13.5|  3.5|    0.5|       3.0|         0.0|                  1.0|        21.5|                 2.5|      0.0|         14.0|\n",
      "|       2|2023-12-01 00:22:17|2023-12-01 00:30:59|              1|         0.66|         1|             false|          79|          79|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.2|                 2.5|      0.0|          9.0|\n",
      "|       2|2023-12-01 00:18:16|2023-12-01 00:25:32|              2|          2.2|         1|             false|         229|         263|           1|       11.4|  1.0|    0.5|       2.0|         0.0|                  1.0|        18.4|                 2.5|      0.0|          7.0|\n",
      "|       2|2023-12-01 00:17:09|2023-12-01 00:33:31|              1|         5.33|         1|             false|          45|         162|           1|       24.7|  1.0|    0.5|       3.0|         0.0|                  1.0|        32.7|                 2.5|      0.0|         16.0|\n",
      "|       2|2023-12-01 00:40:49|2023-12-01 00:44:10|              1|         0.76|         1|             false|         170|         107|           1|        5.8|  1.0|    0.5|       1.0|         0.0|                  1.0|        11.8|                 2.5|      0.0|          3.0|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert pickup and dropoff timestamps to string format\n",
    "combined = combined.withColumn(\"pickup_str\", F.col(\"pickup_datetime\").cast(\"string\"))\n",
    "combined = combined.withColumn(\"dropoff_str\", F.col(\"dropoff_datetime\").cast(\"string\"))\n",
    "\n",
    "# Step 2: Convert the strings back to timestamps\n",
    "combined = combined.withColumn(\"pickup_ts\", F.to_timestamp(\"pickup_str\"))\n",
    "combined = combined.withColumn(\"dropoff_ts\", F.to_timestamp(\"dropoff_str\"))\n",
    "\n",
    "# Step 3: Convert the timestamps to long (Unix epoch seconds)\n",
    "combined = combined.withColumn(\"pickup_long\", F.col(\"pickup_ts\").cast(\"long\"))\n",
    "combined = combined.withColumn(\"dropoff_long\", F.col(\"dropoff_ts\").cast(\"long\"))\n",
    "\n",
    "# Step 4: Calculate the trip duration in minutes\n",
    "combined = combined.withColumn(\"trip_duration\", F.round((F.col(\"dropoff_long\") - F.col(\"pickup_long\")) / 60))\n",
    "\n",
    "# Drop intermediate columns if no longer needed\n",
    "combined = combined.drop(\"pickup_str\", \"dropoff_str\", \"pickup_ts\", \"dropoff_ts\", \"pickup_long\", \"dropoff_long\")\n",
    "\n",
    "# Show the result\n",
    "combined.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = false)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check datatypes\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Northern end of bronx to southern end of staten island is below 50 miles </br>\n",
    "2 hours should be sufficient for travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col as pyspark_col\n",
    "\n",
    "# Filter out rows with trip duration greater than 120 minutes\n",
    "combined = combined.filter(\n",
    "    (pyspark_col(\"trip_duration\") <= 120)\n",
    ")\n",
    "\n",
    "# Filter out rows with trip distance greater than 50 miles \n",
    "combined = combined.filter(\n",
    "    pyspark_col(\"trip_distance\") <= 50\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values & duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dulicate rows\n",
    "combined = combined.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "combined = combined.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly handling by statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# keep 99% quantile of total_amount\n",
    "total_amount_quantile = combined.approxQuantile(\"total_amount\", [0.99], 0.01)[0]\n",
    "combined = combined.filter(pyspark_col(\"total_amount\") <= total_amount_quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:==================================>                    (10 + 6) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows remaining after filtering: 82.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check the percentage of rows that remain after filtering\n",
    "original_count = total_count\n",
    "filtered_count = combined.count()\n",
    "percentage_remaining = (filtered_count / original_count) * 100\n",
    "print(f\"Percentage of rows remaining after filtering: {percentage_remaining:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = false)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_duration: double (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- dropoff_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show schema\n",
    "combined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round time to hour for pickup_datetime and dropoff_datetime\n",
    "combined = combined.withColumn(\"pickup_hour\", F.hour(\"pickup_datetime\"))\n",
    "combined = combined.withColumn(\"dropoff_hour\", F.hour(\"dropoff_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date from pickup_datetime and dropoff_datetime\n",
    "combined = combined.withColumn(\"pickup_date\", F.to_date(\"pickup_datetime\"))\n",
    "combined = combined.withColumn(\"dropoff_date\", F.to_date(\"dropoff_datetime\"))\n",
    "\n",
    "# drop pickup_datetime and dropoff_datetime\n",
    "combined = combined.drop(\"pickup_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 535:> (0 + 0) / 54]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[vendor_indexer, ratecode_indexer, payment_type_indexer,\n\u001b[1;32m     17\u001b[0m                             vendor_encoder, ratecode_encoder, payment_type_encoder])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Fit the pipeline and transform the data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(combined)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Drop the original columns after encoding\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 533:>(20 + 8) / 54][Stage 534:> (0 + 0) / 54][Stage 535:> (0 + 0) / 54]\r"
     ]
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Indexing the categorical columns\n",
    "vendor_indexer = StringIndexer(inputCol=\"VendorID\", outputCol=\"VendorID_index\")\n",
    "ratecode_indexer = StringIndexer(inputCol=\"RatecodeID\", outputCol=\"RatecodeID_index\")\n",
    "payment_type_indexer = StringIndexer(inputCol=\"payment_type\", outputCol=\"payment_type_index\")\n",
    "\n",
    "# OneHotEncoding the indexed columns\n",
    "vendor_encoder = OneHotEncoder(inputCol=\"VendorID_index\", outputCol=\"VendorID_vec\")\n",
    "ratecode_encoder = OneHotEncoder(inputCol=\"RatecodeID_index\", outputCol=\"RatecodeID_vec\")\n",
    "payment_type_encoder = OneHotEncoder(inputCol=\"payment_type_index\", outputCol=\"payment_type_vec\")\n",
    "\n",
    "# Creating a pipeline to chain indexers and encoders\n",
    "pipeline = Pipeline(stages=[vendor_indexer, ratecode_indexer, payment_type_indexer,\n",
    "                            vendor_encoder, ratecode_encoder, payment_type_encoder])\n",
    "\n",
    "# Fit the pipeline and transform the data\n",
    "model = pipeline.fit(combined)\n",
    "encoded_df = model.transform(combined)\n",
    "\n",
    "# Drop the original columns after encoding\n",
    "encoded_df = encoded_df.drop(\"VendorID\", \"RatecodeID\", \"payment_type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export file to raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange columns\n",
    "combined = combined.select(\n",
    "    \"VendorID\",\n",
    "    \"pickup_date\",\n",
    "    \"pickup_hour\",\n",
    "    \"dropoff_date\",\n",
    "    \"dropoff_hour\",\n",
    "    \"trip_duration\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"RatecodeID\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"improvement_surcharge\",\n",
    "    \"congestion_surcharge\",\n",
    "    \"ehail_fee\", \n",
    "    \"total_amount\",\n",
    "    \"payment_type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 364:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+------------+------------+-------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+---------+\n",
      "|VendorID|pickup_date|pickup_hour|dropoff_date|dropoff_hour|trip_duration|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|congestion_surcharge|ehail_fee|\n",
      "+--------+-----------+-----------+------------+------------+-------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+---------+\n",
      "|       2| 2023-12-01|          0|  2023-12-01|           0|          9.0|             false|         1|         137|         263|              2|         3.04|       14.2|  1.0|    0.5|      10.0|         0.0|                  1.0|        29.2|           1|                 2.5|      0.0|\n",
      "|       2| 2023-12-01|          0|  2023-12-01|           0|          9.0|             false|         1|         158|          79|              1|         1.22|       10.0|  1.0|    0.5|       1.0|         0.0|                  1.0|        16.0|           1|                 2.5|      0.0|\n",
      "|       2| 2023-12-01|          0|  2023-12-01|           1|         21.0|             false|         1|         141|         129|              1|         5.21|       26.1|  1.0|    0.5|       4.0|         0.0|                  1.0|        35.1|           1|                 2.5|      0.0|\n",
      "|       2| 2023-12-01|          0|  2023-12-01|           0|          6.0|             false|         1|         144|         211|              1|         0.92|        7.9|  1.0|    0.5|      2.58|         0.0|                  1.0|       15.48|           1|                 2.5|      0.0|\n",
      "|       2| 2023-12-01|          0|  2023-12-01|           0|         17.0|             false|         1|         138|         145|              2|         7.76|       31.7|  6.0|    0.5|      7.84|         0.0|                  1.0|       48.79|           1|                 0.0|      0.0|\n",
      "+--------+-----------+-----------+------------+------------+-------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show the result\n",
    "combined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# export the combined data to parquet\n",
    "combined.write.mode(\"overwrite\").parquet(\"../data/raw/tlc_data/combined.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
