{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium \n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = spark.read.parquet(\"../data/curated/tlc_data/first_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping missing values: 38461\n",
      "Number of rows after dropping missing values: 38461\n",
      "Percentage of rows removed: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# compare the % of rows before and after dropping missing values\n",
    "print(f\"Number of rows before dropping missing values: {df.count()}\")\n",
    "print(f\"Number of rows after dropping missing values: {df.dropna().count()}\")\n",
    "print(f\"Percentage of rows removed: {100*(1 - df.dropna().count()/df.count()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    'VendorID', 'passenger_count', 'RatecodeID', 'store_and_fwd_flag',\n",
    "    'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "    'tolls_amount', 'improvement_surcharge', 'total_amount', \n",
    "    'congestion_surcharge', 'ehail_fee', 'DOLocationID', 'DOBorough','dropoff_hour',\n",
    "    'trip_distance', 'trip_duration'\n",
    "]\n",
    "\n",
    "# Dropping all specified columns at once in PySpark\n",
    "df = df.drop(*columns_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- hourly_trip_count: long (nullable = true)\n",
      " |-- Number of Events: long (nullable = true)\n",
      " |-- CIG: double (nullable = true)\n",
      " |-- WND: double (nullable = true)\n",
      " |-- VIS: double (nullable = true)\n",
      " |-- TMP: double (nullable = true)\n",
      " |-- DEW: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training (60%), validation (20%), and test (20%)\n",
    "train_df, validation_df, test_df = df.randomSplit([0.6, 0.2, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-----------+-----------------+----------------+---+---+---+---+---+---+\n",
      "|datetime|PUBorough|pickup_date|pickup_hour|hourly_trip_count|Number of Events|CIG|WND|VIS|TMP|DEW|SLP|\n",
      "+--------+---------+-----------+-----------+-----------------+----------------+---+---+---+---+---+---+\n",
      "|       0|        0|          0|          0|                0|               0|  0|  0|  0|  0|  0|  0|\n",
      "+--------+---------+-----------+-----------+-----------------+----------------+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in train_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical and numerical columns\n",
    "categorical_columns = ['PUBorough'] \n",
    "numerical_columns = [\n",
    "    'pickup_hour', \n",
    "    'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP', 'Number of Events'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing and Encoding categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_ohe\") for col in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'pickup_hour', \n",
    "        'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP', 'Number of Events',\n",
    "        'PUBorough_ohe'\n",
    "    ], \n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"hourly_trip_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 22:22:28 WARN Instrumentation: [9620d3ba] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/29 22:22:28 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/29 22:22:28 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/08/29 22:22:28 WARN Instrumentation: [9620d3ba] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training set\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean hourly trip count: 818.7936528441631\n"
     ]
    }
   ],
   "source": [
    "# Mean hourly trip count\n",
    "mean_hourly_trip_count = train_df.agg(F.mean(\"hourly_trip_count\")).head()[0]\n",
    "print(f\"Mean hourly trip count: {mean_hourly_trip_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 889.5794710544728\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validation_predictions = model.transform(validation_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"hourly_trip_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data = {validation_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 892.2997045096371\n"
     ]
    }
   ],
   "source": [
    "# If satisfied with validation performance, evaluate on the test set\n",
    "test_predictions = model.transform(test_df)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare varaince of mean hourly count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=47987Kb max_used=47999Kb free=83084Kb\n",
      " bounds [0x00000001089e8000, 0x000000010b928000, 0x00000001109e8000]\n",
      " total_blobs=16984 nmethods=15040 adapters=1855\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosted Trees Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"scaled_features\", labelCol=\"hourly_trip_count\", maxIter=100, maxDepth=5)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, gbt])\n",
    "\n",
    "# Fit the model on the training set\n",
    "model = pipeline.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 495.8899650269797\n",
      "Root Mean Squared Error (RMSE) on test data = 485.31324130901976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validation_predictions = model.transform(validation_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"hourly_trip_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data = {validation_rmse}\")\n",
    "\n",
    "# If satisfied with validation performance, evaluate on the test set\n",
    "test_predictions = model.transform(test_df)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify which features are most important in making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/inspection/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_partial_dependence\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor \u001b[38;5;28;01mas\u001b[39;00m SklearnRandomForestRegressor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_partial_dependence' from 'sklearn.inspection' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/inspection/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.ensemble import RandomForestRegressor as SklearnRandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named clean_df\n",
    "\n",
    "# List of features to be used for prediction\n",
    "feature_columns = ['passenger_count', 'trip_distance', 'trip_duration', 'RatecodeID', \n",
    "                   'hourly_trip_count', 'daily_trip_count', 'CIG', 'WND', 'VIS', \n",
    "                   'TMP', 'DEW', 'SLP', 'Number of Events', 'Day of Week']\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "\n",
    "# Define the Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='hourly_trip_count')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = clean_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Extract the feature and label columns to Pandas\n",
    "pandas_df = test_data.select(feature_columns + ['hourly_trip_count']).toPandas()\n",
    "\n",
    "# Train a RandomForestRegressor model using sklearn (since PDP is not available in PySpark)\n",
    "X = pandas_df[feature_columns]\n",
    "y = pandas_df['hourly_trip_count']\n",
    "\n",
    "sklearn_rf = SklearnRandomForestRegressor(n_estimators=100)\n",
    "sklearn_rf.fit(X, y)\n",
    "\n",
    "# Plot Partial Dependence Plots for a few features\n",
    "features_to_plot = ['trip_distance', 'trip_duration', 'Number of Events']  # Select features you want to plot\n",
    "\n",
    "plot_partial_dependence(sklearn_rf, X, features_to_plot, grid_resolution=50)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
