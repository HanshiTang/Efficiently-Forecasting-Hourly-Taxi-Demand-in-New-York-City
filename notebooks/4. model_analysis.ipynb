{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:===================================================>      (8 + 1) / 9]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium \n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 18:28:47 WARN Utils: Your hostname, Hanshis-Laptop.local resolves to a loopback address: 127.0.0.1; using 172.16.119.20 instead (on interface en0)\n",
      "24/08/29 18:28:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/29 18:28:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = spark.read.parquet(\"../data/curated/tlc_data/first_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 18:28:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping missing values: 21586888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping missing values: 21586888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows removed: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# compare the % of rows before and after dropping missing values\n",
    "print(f\"Number of rows before dropping missing values: {df.count()}\")\n",
    "print(f\"Number of rows after dropping missing values: {df.dropna().count()}\")\n",
    "print(f\"Percentage of rows removed: {100*(1 - df.dropna().count()/df.count()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    'VendorID', 'passenger_count', 'RatecodeID', 'store_and_fwd_flag',\n",
    "    'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "    'tolls_amount', 'improvement_surcharge', 'total_amount', \n",
    "    'congestion_surcharge', 'ehail_fee', 'DOLocationID', 'DOBorough','dropoff_hour',\n",
    "    'trip_distance', 'trip_duration'\n",
    "]\n",
    "\n",
    "# Dropping all specified columns at once in PySpark\n",
    "df = df.drop(*columns_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_date: date (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- hourly_trip_count: long (nullable = true)\n",
      " |-- daily_trip_count: long (nullable = true)\n",
      " |-- CIG: double (nullable = true)\n",
      " |-- WND: double (nullable = true)\n",
      " |-- VIS: double (nullable = true)\n",
      " |-- TMP: double (nullable = true)\n",
      " |-- DEW: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- Number of Events: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training (60%), validation (20%), and test (20%)\n",
    "train_df, validation_df, test_df = df.randomSplit([0.6, 0.2, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+------------+---------+-----------------+----------------+---+---+---+---+---+---+----------------+\n",
      "|PULocationID|pickup_date|pickup_hour|dropoff_date|PUBorough|hourly_trip_count|daily_trip_count|CIG|WND|VIS|TMP|DEW|SLP|Number of Events|\n",
      "+------------+-----------+-----------+------------+---------+-----------------+----------------+---+---+---+---+---+---+----------------+\n",
      "|           0|          0|          0|           0|        0|                0|               0|  0|  0|  0|  0|  0|  0|               0|\n",
      "+------------+-----------+-----------+------------+---------+-----------------+----------------+---+---+---+---+---+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in train_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DOLocationID and PULocationID to string\n",
    "train_df = train_df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"string\"))\n",
    "validation_df = validation_df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"string\"))\n",
    "test_df = test_df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical and numerical columns\n",
    "categorical_columns = ['PUBorough', 'PULocationID'] \n",
    "numerical_columns = [\n",
    "    'pickup_hour', \n",
    "    'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP', 'Number of Events'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing and Encoding categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_ohe\") for col in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'pickup_hour', \n",
    "        'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP', 'Number of Events',\n",
    "        'PUBorough_ohe', 'PULocationID_ohe'\n",
    "    ], \n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"hourly_trip_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 18:29:29 WARN Instrumentation: [06c2e25a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/29 18:29:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/29 18:29:43 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/08/29 18:29:43 WARN Instrumentation: [06c2e25a] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training set\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean hourly trip count: 5005.513692548508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Mean hourly trip count\n",
    "mean_hourly_trip_count = train_df.agg(F.mean(\"hourly_trip_count\")).head()[0]\n",
    "print(f\"Mean hourly trip count: {mean_hourly_trip_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 1250.82091726507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validation_predictions = model.transform(validation_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"hourly_trip_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data = {validation_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1251.5906354522426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# If satisfied with validation performance, evaluate on the test set\n",
    "test_predictions = model.transform(test_df)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosted Trees Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"scaled_features\", labelCol=\"hourly_trip_count\", maxIter=100, maxDepth=5)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, gbt])\n",
    "\n",
    "# Fit the model on the training set\n",
    "model = pipeline.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validation_predictions = model.transform(validation_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"hourly_trip_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data = {validation_rmse}\")\n",
    "\n",
    "# If satisfied with validation performance, evaluate on the test set\n",
    "test_predictions = model.transform(test_df)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify which features are most important in making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
