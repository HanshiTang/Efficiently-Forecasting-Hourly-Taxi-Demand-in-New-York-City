{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import geopandas as gpd\n",
    "import folium \n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/30 11:29:14 WARN Utils: Your hostname, Hanshis-Laptop.local resolves to a loopback address: 127.0.0.1; using 172.16.119.21 instead (on interface en0)\n",
      "24/08/30 11:29:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/30 11:29:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session with increased memory allocation\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ADS Project1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Set the driver memory to 8GB\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Set the executor memory to 8GB\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = spark.read.parquet(\"../data/curated/tlc_data/first_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping missing values: 38461\n",
      "Number of rows after dropping missing values: 38461\n",
      "Percentage of rows removed: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# compare the % of rows before and after dropping missing values\n",
    "print(f\"Number of rows before dropping missing values: {df.count()}\")\n",
    "print(f\"Number of rows after dropping missing values: {df.dropna().count()}\")\n",
    "print(f\"Percentage of rows removed: {100*(1 - df.dropna().count()/df.count()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    'VendorID', 'passenger_count', 'RatecodeID', 'store_and_fwd_flag',\n",
    "    'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "    'tolls_amount', 'improvement_surcharge', 'total_amount', \n",
    "    'congestion_surcharge', 'ehail_fee', 'DOLocationID', 'DOBorough','dropoff_hour',\n",
    "    'trip_distance', 'trip_duration'\n",
    "]\n",
    "\n",
    "# Dropping all specified columns at once in PySpark\n",
    "df = df.drop(*columns_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- hourly_trip_count: long (nullable = true)\n",
      " |-- Number of Events: long (nullable = true)\n",
      " |-- CIG: double (nullable = true)\n",
      " |-- WND: double (nullable = true)\n",
      " |-- VIS: double (nullable = true)\n",
      " |-- TMP: double (nullable = true)\n",
      " |-- DEW: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, month\n",
    "\n",
    "# Filter the data by months for training, validation, and test sets\n",
    "train_df = df.filter((month(col(\"pickup_date\")).isin(7, 8, 9, 10)))\n",
    "validation_df = df.filter((month(col(\"pickup_date\")) == 11))\n",
    "test_df = df.filter((month(col(\"pickup_date\")) == 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-----------+-----------------+----------------+---+---+---+---+---+---+\n",
      "|datetime|PUBorough|pickup_date|pickup_hour|hourly_trip_count|Number of Events|CIG|WND|VIS|TMP|DEW|SLP|\n",
      "+--------+---------+-----------+-----------+-----------------+----------------+---+---+---+---+---+---+\n",
      "|       0|        0|          0|          0|                0|               0|  0|  0|  0|  0|  0|  0|\n",
      "+--------+---------+-----------+-----------+-----------------+----------------+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in train_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical and numerical columns\n",
    "categorical_columns = ['PUBorough'] \n",
    "numerical_columns = [\n",
    "    'pickup_hour', \n",
    "    'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP', 'Number of Events'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing and Encoding categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_ohe\") for col in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'pickup_hour', \n",
    "        'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP', 'Number of Events',\n",
    "        'PUBorough_ohe'\n",
    "    ], \n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"hourly_trip_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/30 11:29:20 WARN Instrumentation: [af88d9d3] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/30 11:29:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/30 11:29:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/08/30 11:29:21 WARN Instrumentation: [af88d9d3] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training set\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean hourly trip count: 792.0786824907984\n"
     ]
    }
   ],
   "source": [
    "# Mean hourly trip count\n",
    "mean_hourly_trip_count = train_df.agg(F.mean(\"hourly_trip_count\")).head()[0]\n",
    "print(f\"Mean hourly trip count: {mean_hourly_trip_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 1001.8514312623018\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validation_predictions = model.transform(validation_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"hourly_trip_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data = {validation_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1056.3164933934179\n"
     ]
    }
   ],
   "source": [
    "# If satisfied with validation performance, evaluate on the test set\n",
    "test_predictions = model.transform(test_df)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model.stages[-1].coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|         Feature|        Coefficient|\n",
      "+----------------+-------------------+\n",
      "|     pickup_hour|   240.588266095392|\n",
      "|   PUBorough_ohe|-162.25873979150455|\n",
      "|Number of Events| 113.79127890935905|\n",
      "|             TMP| -35.31537551810905|\n",
      "|             SLP| -30.41578049482769|\n",
      "|             VIS|-19.862727762080056|\n",
      "|             CIG|-16.021166789717938|\n",
      "|             WND|-10.783820746885791|\n",
      "|             DEW| -6.425061637167243|\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of features used\n",
    "features = [\n",
    "    'pickup_hour', 'CIG', 'WND', 'VIS', 'TMP', 'DEW', 'SLP', 'Number of Events', 'PUBorough_ohe'\n",
    "]\n",
    "\n",
    "# Combine features with their corresponding coefficients\n",
    "feature_coefficients = [(features[i], float(coefficients[i])) for i in range(len(features))]\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"Feature\", StringType(), True),\n",
    "    StructField(\"Coefficient\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Convert coefficients to a DataFrame with the specified schema\n",
    "feature_coefficients_df = spark.createDataFrame(feature_coefficients, schema=schema)\n",
    "\n",
    "# Show the coefficients sorted by their absolute value\n",
    "feature_coefficients_df.orderBy(abs(feature_coefficients_df.Coefficient), ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=44544Kb max_used=44544Kb free=86527Kb\n",
      " bounds [0x00000001089e8000, 0x000000010b5a8000, 0x00000001109e8000]\n",
      " total_blobs=16516 nmethods=14582 adapters=1843\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosted Trees Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"scaled_features\", labelCol=\"hourly_trip_count\", maxIter=100, maxDepth=5)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, gbt])\n",
    "\n",
    "# Fit the model on the training set\n",
    "model = pipeline.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 901.8489930247788\n",
      "Root Mean Squared Error (RMSE) on test data = 1032.6366294357138\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validation_predictions = model.transform(validation_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"hourly_trip_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data = {validation_rmse}\")\n",
    "\n",
    "# If satisfied with validation performance, evaluate on the test set\n",
    "test_predictions = model.transform(test_df)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
